{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from edgar import Company\n",
    "import re\n",
    "from matplotlib import pyplot as pp\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and get unique ciks\n",
    "These come from https://www.kaggle.com/finnhub/sec-filings?select=2021.QTR1.csv which has data back to '94!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/AllEdgarFilings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_files = [\n",
    "    '2020.QTR1.csv',\n",
    "    '2020.QTR2.csv',\n",
    "    '2020.QTR3.csv',\n",
    "    '2020.QTR4.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data from finhub sec filings\n",
    "ciks = []\n",
    "for fn in read_files:\n",
    "    frame = pd.read_csv('2020.QTR1.csv')\n",
    "    ciks.append(frame[frame.form=='10-K'].loc[:, ['symbol', 'cik']])\n",
    "ciks = pd.concat(ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null data\n",
    "ciks = ciks[~pd.isnull(ciks['symbol'])]\n",
    "ciks = ciks[~pd.isnull(ciks['cik'])]\n",
    "\n",
    "# multiple dupicate filings so drop these\n",
    "ciks = ciks.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have ciks for all unique 10-K filings in 2020! There are about 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173370</th>\n",
       "      <td>AA</td>\n",
       "      <td>1675149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185417</th>\n",
       "      <td>AAAU</td>\n",
       "      <td>1708646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252146</th>\n",
       "      <td>AAL</td>\n",
       "      <td>6201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131120</th>\n",
       "      <td>AAMC</td>\n",
       "      <td>1555074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275582</th>\n",
       "      <td>AAME</td>\n",
       "      <td>8177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69221</th>\n",
       "      <td>ZVO</td>\n",
       "      <td>1305323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105268</th>\n",
       "      <td>ZYJT</td>\n",
       "      <td>1442101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94581</th>\n",
       "      <td>ZYME</td>\n",
       "      <td>1403752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152428</th>\n",
       "      <td>ZYNE</td>\n",
       "      <td>1621443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282656</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>846475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4065 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol      cik\n",
       "173370     AA  1675149\n",
       "185417   AAAU  1708646\n",
       "252146    AAL     6201\n",
       "131120   AAMC  1555074\n",
       "275582   AAME     8177\n",
       "...       ...      ...\n",
       "69221     ZVO  1305323\n",
       "105268   ZYJT  1442101\n",
       "94581    ZYME  1403752\n",
       "152428   ZYNE  1621443\n",
       "282656   ZYXI   846475\n",
       "\n",
       "[4065 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciks.sort_values('symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4065"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(ciks['cik'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clearly some tickers have multiple ciks\n",
    "len(np.unique(ciks['symbol'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get business description data for ciks\n",
    "We download data from EDGAR via package edgar using the ciks determined above. Then we look through the raw text to find the business description and pull it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sequence(seq, all_tokens):\n",
    "    check_num = 0\n",
    "    seq_loc = 0\n",
    "    for ll, ss in enumerate(all_tokens):\n",
    "        if ss.lower() in seq:\n",
    "            check_num += 1\n",
    "        else:\n",
    "            check_num = 0\n",
    "        # highly hacky as first location is in table of contents; this return second\n",
    "        if check_num == len(seq):\n",
    "            seq_loc = ll + 1\n",
    "    return seq_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "bds = {}\n",
    "try_starts = [\n",
    "    {'item', '1.', 'business'},\n",
    "    {'item', '1.', 'our', 'business'},\n",
    "    {'item', '1:', 'business'},\n",
    "    {'item', '1.', 'business.'},\n",
    "    {'item', '1', 'business'},\n",
    "    {'item', '1.business'},\n",
    "    \n",
    "        ]\n",
    "try_ends = [\n",
    "    {'item', '1a.', 'risk', 'factors'},\n",
    "    {'item', '1a:', 'risk', 'factors'},\n",
    "    {'item', '1a.', 'risk', 'factors.'},\n",
    "    {'item', '1a', 'risk', 'factors'},\n",
    "        ]\n",
    "max_tries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_over = [(x, y) for x, y in zip(ciks['symbol'].values, ciks['cik'].values)]\n",
    "for ticker, cik in loop_over:\n",
    "    print(f'Extracting {ticker}:{cik}')\n",
    "    tt = 0\n",
    "    while tt < max_tries:\n",
    "        try:\n",
    "            company = Company(ticker, str(cik))\n",
    "            tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "            docs = Company.get_documents(tree, no_of_documents=5)\n",
    "            break\n",
    "        except:\n",
    "            tt+=1\n",
    "    if tt == max_tries:\n",
    "        print(f'Edgar timeout for {ticker}')\n",
    "        continue\n",
    "    for doc in docs:\n",
    "        fs_text = doc.text_content()\n",
    "        # could tokenise better here\n",
    "        # re.split(r'[ \\t\\n]+', fs_text) would split on any number of tabs, whitespaces, newlines\n",
    "        fs_text = fs_text.replace('\\t', ' ')\n",
    "        fs_text = fs_text.replace('\\n', ' ')\n",
    "        tokenised = fs_text.split()\n",
    "        \n",
    "        for ts in try_starts:\n",
    "            start_seq = find_sequence(ts, tokenised)\n",
    "            if start_seq != 0:\n",
    "                break\n",
    "        for te in try_ends:\n",
    "            end_seq = find_sequence(te, tokenised)\n",
    "            if end_seq != 0:\n",
    "                break\n",
    "        checks = (start_seq != 0) & (end_seq != 0) & ((end_seq - start_seq) > 100)\n",
    "        if checks:\n",
    "            print(f'from {start_seq} to {end_seq} out of {len(tokenised)}')\n",
    "            break\n",
    "    if checks:\n",
    "        bds[f'{ticker}:{cik}'] = tokenised[start_seq:end_seq]\n",
    "    else:\n",
    "        bds[f'{ticker}:{cik}'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bds, open(\"bds_1.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/')\n",
    "file1 = open(\"bds_1.txt\",\"a\")\n",
    "for kk, vv in bds.items():\n",
    "    file1.write(kk + '\\n')\n",
    "    file1.write(' '.join(vv)+ '\\n')\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for vv in bds.values():\n",
    "    if len(vv) > 0:\n",
    "        i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(cik, doc_num):\n",
    "    company = Company(str(cik), str(cik))\n",
    "    tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "    docs = Company.get_documents(tree, no_of_documents=doc_num+1)\n",
    "    fs_text = doc.text_content()\n",
    "    fs_text = fs_text.replace('\\t', ' ')\n",
    "    fs_text = fs_text.replace('\\n', ' ')\n",
    "    tokenised = fs_text.split()\n",
    "    print(' '.join(tokenised)[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(1000753, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly some extractions have failed, about 40% currently which is too high. These will require a case by case handling. Below is a problem for novavax. Also sometimes edgar failes as per error message abover (we could just try catch this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding start sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(cik, doc_num):\n",
    "    company = Company(str(cik), str(cik))\n",
    "    tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "    docs = Company.get_documents(tree, no_of_documents=doc_num+1)\n",
    "    fs_text = doc.text_content()\n",
    "    fs_text = fs_text.replace('\\t', ' ')\n",
    "    fs_text = fs_text.replace('\\n', ' ')\n",
    "    return fs_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = get_tokens(1000697, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smoothed(tokens, string, smoothing):\n",
    "    contains = np.array([x.lower().find(string) != -1 for x in tokens])\n",
    "    return np.minimum(np.convolve(contains, np.ones(smoothing)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 3\n",
    "loc_business = find_smoothed(all_tokens, 'business', win)\n",
    "loc_item = find_smoothed(all_tokens, 'item', win)\n",
    "loc_1 = find_smoothed(all_tokens, '1', win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(loc_business+loc_item+loc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win=3\n",
    "loc_item = find_smoothed(all_tokens, 'item', win)\n",
    "loc_1a = find_smoothed(all_tokens, '1a', win)\n",
    "loc_risk = find_smoothed(all_tokens, 'risk', win)\n",
    "loc_factors = find_smoothed(all_tokens, 'factors', win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(loc_item+ loc_1a+loc_risk+loc_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
