{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.linear_model import Ridge, GammaRegressor\n",
    "\n",
    "# PyTorch stuff\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autoselect target device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "os.chdir('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/nlp-group-project-main/nvdm')\n",
    "from models import nvdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "## Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of Lemmatizer\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Process a single line of text. \"\"\"\n",
    "\n",
    "    # Strip trailing characters if any (e.g. newline)\n",
    "    text_new = text.strip()\n",
    "    \n",
    "    # Remove puncuation\n",
    "    text_new = ''.join(ch for ch in text_new if ch not in string.punctuation)\n",
    "\n",
    "    # Lower case\n",
    "    text_new = text_new.lower()\n",
    "    \n",
    "    # Tokenise by space\n",
    "    tokens = text_new.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in STOPWORDS]\n",
    "\n",
    "    # Lemmatise each word\n",
    "    tokens = [LEMMATIZER.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    text_new = ' '.join(tokens)\n",
    "\n",
    "    return text_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\" Base class for clustering models.\n",
    "    \n",
    "    Basically a wrapper for a variety of models.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\" Train the model. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\" Apply model to new data.\n",
    "        \n",
    "        Should output a topic-document matrix,\n",
    "        where each element is a score indicating how likely the document\n",
    "        should be assigned to the topic.\n",
    "        For sklearn LDA, transform() does this by default.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def topic_vocab_matrix(self):\n",
    "        \"\"\" Each model should be able to return a topic-vocab matrix\n",
    "        containing a score (e.g. probability) of a word in the vocabulary\n",
    "        occuring in the k^th topic. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # TODO: topic-document matrix\n",
    "    # TODO: perplexity? Don't think K-Means has a notion of perplexity\n",
    "    # (because we need probabilities).\n",
    "\n",
    "    \n",
    "class KMeansModel(Model):\n",
    "    \"\"\" Wrapper for scikit-learn KMeans. \"\"\"\n",
    "    def __init__(self, num_topics):\n",
    "        self.model = KMeans(\n",
    "            n_clusters = num_topics,\n",
    "            init='k-means++',\n",
    "            max_iter = 300,\n",
    "            n_init = 10,\n",
    "            verbose = False)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.model.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\" Returns a topic-document matrix of distances per cluster. \"\"\"\n",
    "        return -self.model.transform(X)\n",
    "    \n",
    "    @property\n",
    "    def topic_vocab_matrix(self):\n",
    "        \"\"\" Return K-Means clusters.\n",
    "        \n",
    "        ndarray of shape (num_topics, n_features)\n",
    "        \"\"\"\n",
    "        return self.model.cluster_centers_\n",
    "\n",
    "\n",
    "class LDAModel(Model):\n",
    "    \"\"\" Wrapper for scikit-learn LDA. \"\"\"\n",
    "    def __init__(self, num_topics):\n",
    "        self.model = LatentDirichletAllocation(\n",
    "            n_components=num_topics,\n",
    "            max_iter=5,\n",
    "            learning_method='online',\n",
    "            learning_offset=50.,\n",
    "            n_jobs=-1)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.model.fit(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\" Returns a topic-document matrix of probabilities. \"\"\"\n",
    "        return self.model.transform(X) \n",
    "    \n",
    "    @property\n",
    "    def topic_vocab_matrix(self):\n",
    "        \"\"\" Gets the components_ attribute of LDA, normalized\n",
    "        \n",
    "        Quoting sklearn docs:\n",
    "        Variational parameters for topic word distribution.\n",
    "        Since the complete conditional for topic word distribution is a Dirichlet,\n",
    "        components_[i, j] can be viewed as pseudocount that represents\n",
    "        the number of times word j was assigned to topic i.\n",
    "        It can also be viewed as distribution over the words for each topic after normalization:\n",
    "        model.components_ / model.components_.sum(axis=1)[:, np.newaxis].\n",
    "        \"\"\"\n",
    "        # return self.model.components_\n",
    "        return self.model.components_ / self.model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "class NVDMModel(Model):\n",
    "    \"\"\" PyTorch NVDM model.\n",
    "    \n",
    "    Loads a pretrained model from disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, vocab_size, hidden_size=500, num_topics=300):\n",
    "        self.model = NVDM(vocab_size, hidden_size, num_topics, 1, \"cpu\")\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "        self.model.eval()\n",
    "        \n",
    "        decoder = self.model.decoder[0]\n",
    "        weights = decoder.weight.data.detach().clone().cpu().numpy()\n",
    "        self.topic_vocab = weights.T\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\" We don't train the model here because it takes too long. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\" Output a topic-document matrix. \"\"\"\n",
    "        n_doc, n_vocab = X.shape\n",
    "        n_topic = self.topic_vocab.shape[0]\n",
    "        \n",
    "        # shape (n_doc, n_topic)\n",
    "        # Score of each document for a topic is the average scores\n",
    "        # of the document's words in the topic.\n",
    "        topic_doc = X @ self.topic_vocab.T\n",
    "        \n",
    "        # Optionally, normalize by document length.\n",
    "        topic_doc = topic_doc / X.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return topic_doc\n",
    "    \n",
    "    @property\n",
    "    def topic_vocab_matrix(self):\n",
    "        \"\"\" Returns the learned semantic embeddings of each word. \"\"\"\n",
    "        return self.topic_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic coherence.\n",
    "def umass_score(tf):\n",
    "    \"\"\" Compute topic coherence using UMass metric.\n",
    "    \n",
    "    Ref: http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "    \n",
    "    tf: term-frequency matrix for each document.\n",
    "        Each i^th row is the BOW representation of the i^th document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # D(wi): count of documents containing the word wi (i.e. df)\n",
    "    Dwi = np.array(np.sum(tf > 0, axis=0))[0]\n",
    "\n",
    "    W_bin = np.zeros_like(tf)\n",
    "    W_bin[tf > 0] = 1\n",
    "    \n",
    "    # D(wi, wj): count of documents containing both words wi and wj\n",
    "    Dwi_wj = W_bin.T @ W_bin\n",
    "\n",
    "    score_umass = np.log((Dwi_wj + 1)/ Dwi)\n",
    "    \n",
    "    return score_umass\n",
    "\n",
    "def topic_coherence(topic_vocab, n_top_words, pair_score):\n",
    "    \"\"\" Compute the topic coherence of each topic,\n",
    "    given a learned topic-vocabulary matrix, the number of top words to use\n",
    "    and a matrix of pairwise scores (e.g. umass_score output)\n",
    "    \n",
    "    topic_vocab: dimensions (number of topics, vocabulary size).\n",
    "    model.components_ for LDA, and the \"semantic embedding\" matrix in the decoder for NVDM.\n",
    "    \n",
    "    pair_score: matrix of scores (e.g. UMass)\n",
    "    \"\"\"\n",
    "    coherences = []\n",
    "    for topic_idx, topic in enumerate(topic_vocab):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        coh = 0\n",
    "        for i in range(len(top_features_ind)):\n",
    "            for j in range(i):\n",
    "                coh += pair_score[top_features_ind[i], top_features_ind[j]]\n",
    "        coherences.append(coh)\n",
    "    return coherences\n",
    "\n",
    "def plot_top_words(topic_vocab, feature_names, n_top_words, title):\n",
    "    \"\"\" Given a topic-vocabulary matrix containing scores\n",
    "    (e.g. probabilities, higher the better),\n",
    "    plot the top words as a frequency bar-graph for each topic.\n",
    "    \n",
    "    e.g. set topic_vocab=model._components for LDA.\n",
    "    \"\"\"\n",
    "    K = len(topic_vocab)\n",
    "    n_x = 5\n",
    "    n_y = int(np.ceil(K / n_x))\n",
    "    fig, axes = plt.subplots(n_y, n_x, figsize=(2.5 * n_x, 4 * n_y), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(topic_vocab):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 14})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    # plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def daily_adjusted_r2(returns, features):\n",
    "    \"\"\"\n",
    "    Takes in a dates by stocks matrix (T x M) of stock returns and a stocks by features matrix (M x K) \n",
    "    of features.\n",
    "    \n",
    "    Loops over dates. On each date, performs a OLS regression of M stock returns against a M x (K + 1) matrix\n",
    "    of features, including an intercept which has been added. Then records the adjusted R2 of that regression\n",
    "    on that date.\n",
    "    \n",
    "    Outputs a dataframe of length T of R2 values.\n",
    "    \"\"\"\n",
    "    all_dates = returns.index\n",
    "    adj_r2 = []\n",
    "\n",
    "    for dd in all_dates:\n",
    "        # removing features we don't have returns for\n",
    "        reg_data = returns.loc[[dd]].transpose().join(features).dropna(axis=0).values\n",
    "        y = reg_data[:, 0]\n",
    "        X = reg_data[:, 1:]\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "        X = std_scaler.fit_transform(X)\n",
    "\n",
    "        X = sm.add_constant(X, prepend=False)\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        res = ols_model.fit()\n",
    "        adj_r2.append(res.rsquared_adj)\n",
    "    return pd.DataFrame(index=all_dates, data=adj_r2)\n",
    "\n",
    "\n",
    "def cv_score_ridge(dependent, features):\n",
    "    trials = 200\n",
    "    alphas = [0.01, 0.1, 1, 10, 100, 200, 500, 1000]\n",
    "    best_score = 0\n",
    "    stdev_best = 0\n",
    "    alpha_best = 0\n",
    "    for alpha in alphas:\n",
    "        res_alpha = []\n",
    "        for _ in range(trials):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, dependent)\n",
    "            my_regression = Ridge(alpha=alpha)\n",
    "            std_scaler = StandardScaler()\n",
    "            X_train = std_scaler.fit_transform(X_train)\n",
    "            X_test = std_scaler.transform(X_test)\n",
    "            my_regression.fit(X_train, y_train)\n",
    "            res_alpha.append(my_regression.score(X_test, y_test))\n",
    "        if np.mean(res_alpha) > best_score:\n",
    "            best_score = np.mean(res_alpha)\n",
    "            stdev_best = np.std(res_alpha)\n",
    "            alpha_best = alpha\n",
    "    return best_score, stdev_best, alpha_best\n",
    "\n",
    "\n",
    "def cv_score_gamma(dependent, features):\n",
    "    trials = 200\n",
    "    alphas = [0.01, 0.1, 1, 10, 100, 200, 500, 1000]\n",
    "    best_score = 0\n",
    "    stdev_best = 0\n",
    "    alpha_best = 0\n",
    "    for alpha in alphas:\n",
    "        res_alpha = []\n",
    "        for _ in range(trials):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, dependent)\n",
    "            my_regression = GammaRegressor(alpha=alpha)\n",
    "            std_scaler = StandardScaler()\n",
    "            X_train = std_scaler.fit_transform(X_train)\n",
    "            X_test = std_scaler.transform(X_test)\n",
    "            my_regression.fit(X_train, y_train)\n",
    "            res_alpha.append(my_regression.score(X_test, y_test))\n",
    "        if np.mean(res_alpha) > best_score:\n",
    "            best_score = np.mean(res_alpha)\n",
    "            stdev_best = np.std(res_alpha)\n",
    "            alpha_best = alpha\n",
    "    return best_score, stdev_best, alpha_best  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sp500(path, preprocess=False):\n",
    "    \"\"\" Load S&P500 data from the per-company text files in the supplied directory path.\n",
    "    \n",
    "    Within the directory, each file is named \"<ticker>_<sector>.txt\".\n",
    "    Each contains the business description (BD) of the company.\n",
    "    \n",
    "    If preprocess is True, the preprocess the business descriptions at the same time.\n",
    "    \"\"\"\n",
    "    filenames = os.listdir(path)\n",
    "\n",
    "    tickers = []\n",
    "    sectors = []\n",
    "    bds = []\n",
    "    for fn in filenames:\n",
    "        prefix = fn.split('.txt')[0]\n",
    "        ticker, sector = prefix.split('_')\n",
    "        filepath = os.path.join(path, fn)\n",
    "        with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "            bd = f.read().strip()\n",
    "        \n",
    "        if preprocess:\n",
    "            bd = preprocess_text(bd)\n",
    "\n",
    "        tickers.append(ticker)\n",
    "        sectors.append(sector)\n",
    "        bds.append(bd)\n",
    "    \n",
    "    return tickers, sectors, bds\n",
    "\n",
    "\n",
    "def load_bds1(path, preprocess=False, exclude_tickers=None):\n",
    "    \"\"\" Load data from the business data, given the file path (e.g. \"data/bds_1.txt\").\n",
    "    \n",
    "    In the file, each company has two consecutive lines.\n",
    "    The first line is <company ticker>:<CIK> (we only care about the ticker)\n",
    "    and the second line is the company business description.\n",
    "    \n",
    "    exclude_tickers is a list of tickers that we want to ignore in bds_1.txt.\n",
    "    For example, we can use this to exclude any S&P500 companies to avoid\n",
    "    overlapping of datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    company_ids_all = [ln.strip() for ln in lines[0::2]]\n",
    "    company_descriptions_all = [ln.strip() for ln in lines[1::2]]\n",
    "    company_tickers = [x.split(':')[0] for x in company_ids_all]\n",
    "\n",
    "    exclusion_set = set(exclude_tickers) if exclude_tickers is not None else set()\n",
    "\n",
    "    tickers = []\n",
    "    bds = []\n",
    "    \n",
    "    # Some business descriptions are too short (or even empty),\n",
    "    # so we only keep those with a length (number of characters) deemed reasonable.\n",
    "    bd_valid_length = 3000\n",
    "    for ticker, bd in zip(company_tickers, company_descriptions_all):\n",
    "        if ticker not in exclusion_set and len(bd) >= bd_valid_length:\n",
    "            tickers.append(ticker)\n",
    "            \n",
    "            if preprocess:\n",
    "                bd = preprocess_text(bd)\n",
    "            bds.append(bd)\n",
    "    \n",
    "    return tickers, bds\n",
    "\n",
    "\n",
    "def load_returns(path, tickers, start_d=np.datetime64('2018-01-01'), end_d=np.datetime64('2020-01-01')):\n",
    "    \"\"\"\n",
    "    Loads price data from path, between a set of dates and converts to daily log returns. \n",
    "    \n",
    "    Data only available for S&P 500 stocks that were in index over past 20 years. Supply list of tickers to \n",
    "    subset this.\n",
    "    \n",
    "    Example path:\n",
    "    '/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/MarketData/Price.csv'\n",
    "    \n",
    "    Please install Pandas library to get working (pip install pandas)\n",
    "    \"\"\"\n",
    "    \n",
    "    # loads    \n",
    "    price_data = pd.read_csv(path)\n",
    "    \n",
    "    # subset desired tickers from price file\n",
    "    select_these = np.in1d(price_data.tic.values, list(tickers))\n",
    "    \n",
    "    # price file stored columns wise. Select columns we want.\n",
    "    price_sp50 = price_data.loc[select_these, ['tic', 'datadate', 'prccd']]\n",
    "    \n",
    "    # format dates to use pandas date handling\n",
    "    price_sp50['datadate'] = pd.to_datetime(price_sp50['datadate'], format='%Y%m%d')\n",
    "    \n",
    "    # pivot to dates by stocks table\n",
    "    price_sp50 = pd.pivot_table(price_sp50,index='datadate',columns='tic',values='prccd')\n",
    "    \n",
    "    # fill in missing prices so we can calculate returns over holidays\n",
    "    price_sp50 = price_sp50.ffill(limit=5)\n",
    "    \n",
    "    # subset business days only\n",
    "    business_ds = pd.date_range(start_d, end_d, freq='B')\n",
    "    price_sp50 = price_sp50.reindex(business_ds)\n",
    "    price_sp50 = price_sp50.loc[~np.all(np.isnan(price_sp50.values), axis=1), :]\n",
    "    \n",
    "    # remove stocks with missing data\n",
    "    price_sp50 = price_sp50.dropna(axis=1)\n",
    "    \n",
    "    # we now have a continuous price series for all stocks. We convert these into changes in price, \n",
    "    # i.e. log returns. Logs used as dampens outliers for subsequent OLS regression.\n",
    "    returns_sp50 = np.log(price_sp50) - np.log(price_sp50.shift(1))\n",
    "    return returns_sp50.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/miniconda3/envs/comp0086_Exercises/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "sp500_path = '/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/SP500'\n",
    "sp500_tickers, sp500_sectors, sp500_bds = load_sp500(sp500_path, preprocess=True)\n",
    "\n",
    "bds1_path = '/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/bds_1.txt'\n",
    "bds1_tickers, bds1_bds = load_bds1(bds1_path, preprocess=True, exclude_tickers=sp500_tickers)\n",
    "\n",
    "stock_returns_path = '/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/MarketData/Price.csv'\n",
    "sp500_returns = load_returns(stock_returns_path, sp500_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_returns = pd.DataFrame(sp500_returns.mean(axis=0))\n",
    "total_returns.columns = ['returns']\n",
    "total_stdev = pd.DataFrame(sp500_returns.std(axis=0))\n",
    "total_stdev.columns = ['stdev']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.95, max_features=4000, min_df=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 4000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, max_df=0.95, min_df=2)\n",
    "tf_vectorizer.fit(bds1_bds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sp500 = tf_vectorizer.transform(sp500_bds).toarray()\n",
    "X_bds = tf_vectorizer.transform(bds1_bds).toarray()\n",
    "\n",
    "um_score_bds = umass_score(X_bds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up NVDM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDataset(Dataset):\n",
    "    \"\"\" Very simple dataset object. Stores all the passages.\n",
    "    \n",
    "    This is just for compatibility with PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BDDataset(torch.tensor(X_sp500, dtype=torch.float32))\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "hidden_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, data_loader, outer_epochs=1000, print_every=100, device=\"cpu\"):\n",
    "\n",
    "    # Trains both the encoder and decoder at the same time.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = timer()\n",
    "    print(\"Start training...\")\n",
    "\n",
    "\n",
    "    for epoch in range(outer_epochs):\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        rec_sum = 0.0\n",
    "        kl_sum = 0.0\n",
    "        n = len(data_loader)\n",
    "\n",
    "        for text in data_loader:\n",
    "            text = text.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(text, kl_weight=1.0)\n",
    "            loss = loss_dict[\"total\"].sum()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # For printing\n",
    "            loss_sum += loss.item()\n",
    "            rec_sum += loss_dict[\"rec\"].sum().item()\n",
    "            kl_sum += loss_dict[\"kl\"].sum().item()\n",
    "\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"[Time: {timedelta(seconds=timer() - start_time)}, Epoch {epoch + 1}] Loss {loss_sum/n}, Rec {rec_sum/n}, KL {kl_sum/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/nlp-group-project-main/Models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "kmean = KMeansModel(k)\n",
    "kmean.fit(X_bds)\n",
    "p_file = open(f'KMeans_{k}', 'wb')\n",
    "pickle.dump(kmean.model, p_file)\n",
    "p_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "lda = LDAModel(k)\n",
    "lda.fit(X_bds)\n",
    "p_file = open(f'LDA_{k}', 'wb')\n",
    "pickle.dump(lda.model, p_file)\n",
    "p_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "hidden_size = 250\n",
    "epochs = 100\n",
    "model_nvdm = nvdm.NVDM(n_features, hidden_size, k, 1, device)\n",
    "model_nvdm = model_nvdm.to(device)\n",
    "model_nvdm.train()\n",
    "train(model_nvdm, data_loader, outer_epochs=epochs, print_every=500, device=device)\n",
    "torch.save(model_nvdm, f'NVDM_{k}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
