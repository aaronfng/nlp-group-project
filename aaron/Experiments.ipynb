{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_handling as data\n",
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original data\n",
    "# DATA_RAW_PATH = \"./data/bds_1.txt\"\n",
    "# IDs, BDs = data.load_raw(DATA_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that has already been preprocessed\n",
    "# Generated by applying pp.preprocess_text() to each BD,\n",
    "# then saved to a TSV\n",
    "DATA_CLEAN_PATH = \"./data/bds_1_clean.txt\"\n",
    "IDs, BDs = data.load_raw(DATA_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following PyTorch's tutorial for data setup.\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build frequency table\n",
    "# (cleaned data joins tokens by space)\n",
    "counter = Counter()\n",
    "for desc in BDs:\n",
    "    counter.update(desc.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch torchtext vocabulary converts tokens to indices and vice versa.\n",
    "# Also has an '<unk>' for OOV words (might be useful later).\n",
    "vocab = Vocab(counter,\n",
    "              max_size=None,\n",
    "              min_freq=1,\n",
    "              specials=['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "52244 52244 commercial\n",
      "[52244, 1411, 76, 4061]\n"
     ]
    }
   ],
   "source": [
    "# Example usage: unknown word, convert token to int ID, convert a whole list of tokens to IDs.\n",
    "print(vocab.stoi[\"thisworddoesntexist\"], vocab.unk_index)\n",
    "print(vocab[\"commercial\"], vocab.stoi[\"commercial\"], vocab.itos[vocab[\"commercial\"]])\n",
    "print(vocab.lookup_indices([\"commercial\", \"fact\", \"data\", \"tech\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDataset(Dataset):\n",
    "    \"\"\" Very simple dataset object. Stores all the passages.\n",
    "    \n",
    "    This is just for compatibility with PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Preprocessing\" function\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in text.split(\" \")]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Convert a batch of text (each a list of tokens) into appropriate torch tensors.\n",
    "    \n",
    "    Modification of https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html.\n",
    "    We don't need labels.\n",
    "    \"\"\"\n",
    "    # Offsets tells the model (which will use EmbeddingBag) where each text starts.\n",
    "    text_list, offsets = [], [0]\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BDDataset(BDs)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We medallion financi compani financ compani organ delawar corpor includ medallion bank primari oper subsidiari In recent year strateg growth medallion bank origin consum loan purchas recreat vehicl boat trailer financ home improv We histor lead posit origin acquir servic loan financ taxi medallion variou type commerci busi sinc medallion bank acquir consum loan portfolio began origin consum loan 2004 increas consum loan portfolio compound annual growth rate 16 19 loan sale 2016 2017 2018 In janu\n"
     ]
    }
   ],
   "source": [
    "# How to iterate through the data batchwise\n",
    "for idx, (text, offsets) in enumerate(data_loader):\n",
    "    \n",
    "    # Just have a look at the first item in the first batch.\n",
    "    start = offsets[0]\n",
    "    end = offsets[1]\n",
    "    indices = text[start:end]\n",
    "    \n",
    "    # Should be equal to the 1st original BD text.\n",
    "    tmp = \" \".join(vocab.itos[i] for i in indices)\n",
    "    print(tmp[:500])\n",
    "    assert tmp == BDs[0]\n",
    "    \n",
    "    break\n",
    "    \n",
    "    # TODO: training stuff (model(), loss, backward, optimizer.step etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model\n",
    "# TODO: try denoising autoencoder\n",
    "# example https://github.com/shentianxiao/text-autoencoders/blob/master/model.py\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Model, self).__init__()\n",
    "        # EmbeddingBag is basically Embedding but aggregates words (i.e. bag-of-words).\n",
    "        # See the \"mode\" argument. Quoted from official documentation:\n",
    "        # with ``mode=\"sum\"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.sum(dim=1)``\n",
    "        # Also takes an additional \"offsets\" 1D array, which indicates where each sentence starts in the batch.\n",
    "        # For now we treat a whole document as a sentence (list of tokens).\n",
    "        self.embedding = nn.EmbeddingBag(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            sparse=True,\n",
    "            mode=\"sum\", # can be sum, mean or max (defaults to mean)\n",
    "        )\n",
    "        \n",
    "        # TODO: layers\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        \"\"\" Takes a batch of texts and a 1D array telling us where each sentence starts. \"\"\"\n",
    "        out = self.embedding(text, offsets)\n",
    "        \n",
    "        # TODO: forward pass\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
