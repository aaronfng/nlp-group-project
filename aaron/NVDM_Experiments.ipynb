{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_handling as data\n",
    "import preprocess as pp\n",
    "from models import nvdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original data\n",
    "# DATA_RAW_PATH = \"./data/bds_1.txt\"\n",
    "# IDs, BDs = data.load_raw(DATA_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that has already been preprocessed\n",
    "# Generated by applying pp.preprocess_text() to each BD,\n",
    "# then saved to a TSV\n",
    "DATA_CLEAN_PATH = \"./data/bds_1_clean.txt\"\n",
    "IDs_raw, BDs_raw = data.load_raw(DATA_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 2034\n"
     ]
    }
   ],
   "source": [
    "# Some entries have empty BDs, so filter those out\n",
    "IDs = []\n",
    "BDs = []\n",
    "for iid, bd in zip(IDs_raw, BDs_raw):\n",
    "    if len(bd) > 0:\n",
    "        IDs.append(iid)\n",
    "        BDs.append(bd)\n",
    "\n",
    "print(len(IDs), len(BDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following PyTorch's tutorial for data setup.\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build frequency table\n",
    "# (cleaned data joins tokens by space)\n",
    "counter = Counter()\n",
    "for desc in BDs:\n",
    "    counter.update(desc.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "# PyTorch torchtext vocabulary converts tokens to indices and vice versa.\n",
    "# Also has an '<unk>' for OOV words (might be useful later).\n",
    "vocab = Vocab(counter,\n",
    "              max_size=10000,\n",
    "              min_freq=1,\n",
    "              specials=['<unk>'])\n",
    "print(len(vocab))\n",
    "# actual is 70770 without max_size restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDataset(Dataset):\n",
    "    \"\"\" Very simple dataset object. Stores all the passages.\n",
    "    \n",
    "    This is just for compatibility with PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Preprocessing\" function: just splits the text\n",
    "# The file's text is already preprocessed.\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in text.split(\" \")]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Convert a batch of text (each a list of tokens) into appropriate torch tensors.\n",
    "    \n",
    "    Modification of https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html.\n",
    "    We don't need labels.\n",
    "    \"\"\"\n",
    "    # Offsets tells the model (which will use EmbeddingBag) where each text starts.\n",
    "    text_list, offsets = [], [0]\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader to iterate over dataset in batches during training/evaluation\n",
    "dataset = BDDataset(BDs)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "hidden_size = 500\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "\n",
    "# Total number of epochs\n",
    "outer_epochs = 200\n",
    "\n",
    "# Epochs for training the encoder/decoder on each alternation.\n",
    "inner_epochs = 1\n",
    "\n",
    "model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Separate the encoder from decoder parameters when training in an alternating manner.\n",
    "# Also including linear layers than output mu and log(sigma)\n",
    "# (not the most elegant method but works)\n",
    "optim_encoder = torch.optim.Adam(\n",
    "    list(model.encoder.parameters()) +\n",
    "    list(model.mu.parameters()) +\n",
    "    list(model.log_sigma.parameters()),\n",
    "    lr=0.0001)\n",
    "optim_decoder = torch.optim.Adam(model.decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Trains both the encoder and decoder at the same time.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "for epoch in range(outer_epochs):\n",
    "\n",
    "    # 1. Train the encoder and decoder in turns,\n",
    "    #    fixing the parameters of the other every time.\n",
    "#     for switch in range(0, 2):\n",
    "#         # Author's code trains the decoder first,\n",
    "#         # not sure if the order matters.\n",
    "#         if switch == 0:\n",
    "#             optimizer = optim_encoder\n",
    "#         else:\n",
    "#             optimizer = optim_decoder\n",
    "    \n",
    "    # 2. Train everything together\n",
    "    if True:\n",
    "        # Do training\n",
    "        for alt_epoch in range(inner_epochs):\n",
    "            \n",
    "            loss_sum = 0.0\n",
    "            rec_sum = 0.0\n",
    "            kl_sum = 0.0\n",
    "            n = len(data_loader)\n",
    "            \n",
    "            for idx, (text, offsets) in enumerate(data_loader):\n",
    "                text = text.to(device)\n",
    "                offsets = offsets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                logits, loss_dict = model(text, offsets, kl_weight=1.0)\n",
    "                loss = loss_dict[\"total\"]\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # For printing\n",
    "                loss_sum += loss.item()\n",
    "                rec_sum += loss_dict[\"rec\"].item()\n",
    "                kl_sum += loss_dict[\"kl\"].item()\n",
    "\n",
    "            model_str = \"All\" # \"Enc\" if switch == 0 else \"Dec\"\n",
    "            print(f\"[Time: {timedelta(seconds=timer() - start_time)}, Epoch {epoch + 1}, {model_str} {alt_epoch + 1}] Loss {loss_sum/n}, Rec {rec_sum/n}, KL {kl_sum/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "- The original paper alternates between the encoder and decoder when training. i.e. It trains the decoder first for some (e.g. 10) iterations, fixing the encoders parameters. Then it trains the encoder, fixing the decoder's parameters. This is one epoch, which is repeated some number of times until convergence. However, this results in poor training performance: the KL is observed to fluctuate. The encoder and decoder are unable to jointly converge. By training them all together both the reconstruction loss and KL appear to go down.\n",
    "- Right now we weight the reconstruction and KL losses equally: $L_{total} = L_{rec} + L_{KL}$. We could define a hyperparameter $\\beta$ so that $L_{total} = L_{rec} + \\beta L_{KL}$, which might help balance the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NVDM(\n",
       "  (embed_bow): EmbeddingBag(10001, 10001, mode=sum)\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=10001, out_features=500, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (mu): Linear(in_features=500, out_features=10, bias=True)\n",
       "  (log_sigma): Linear(in_features=500, out_features=10, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=10001, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELSAVE_PATH = \"./modelsaves/nvdm_200epochs.pt\"\n",
    "# torch.save(model.state_dict(), MODELSAVE_PATH)\n",
    "\n",
    "model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "model.load_state_dict(torch.load(MODELSAVE_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10001, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the vocab-topic matrix (known as R in the paper).\n",
    "# It has dimensions |V| x K: vocab size x number of topics\n",
    "decoder = model.decoder[0]\n",
    "weights = decoder.weight.data.detach().clone()\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weapon 6165\n",
      "weapon\t1.0\n",
      "command\t0.9859694838523865\n",
      "alarm\t0.9724387526512146\n",
      "catv\t0.9687132239341736\n",
      "nondisclosur\t0.9681808948516846\n",
      "harsh\t0.9667993783950806\n",
      "microwav\t0.9648943543434143\n",
      "dana\t0.961925208568573\n",
      "surg\t0.959961473941803\n",
      "newest\t0.9583815932273865\n",
      "warfar\t0.9568072557449341\n",
      "lightweight\t0.9552127718925476\n",
      "humid\t0.9520108699798584\n",
      "lockhe\t0.9518946409225464\n",
      "electromagnet\t0.9515492916107178\n",
      "-----------\n",
      "medic 235\n",
      "medic\t1.0\n",
      "health\t0.9257354736328125\n",
      "healthcar\t0.9163874983787537\n",
      "care\t0.8989317417144775\n",
      "afford\t0.8853657841682434\n",
      "bodi\t0.870684027671814\n",
      "hospit\t0.851254403591156\n",
      "patient\t0.8428614735603333\n",
      "often\t0.8355170488357544\n",
      "both\t0.8289807438850403\n",
      "treatment\t0.8267985582351685\n",
      "physician\t0.8263426423072815\n",
      "supplement\t0.8239941596984863\n",
      "fine\t0.8207406997680664\n",
      "age\t0.8182672262191772\n",
      "-----------\n",
      "compani 3\n",
      "compani\t1.0\n",
      "corpor\t0.9685456156730652\n",
      "As\t0.9668803811073303\n",
      "offic\t0.9664745330810547\n",
      "maintain\t0.9538002610206604\n",
      "sourc\t0.9514853358268738\n",
      "factor\t0.9485275745391846\n",
      "meet\t0.9470439553260803\n",
      "make\t0.9467697143554688\n",
      "report\t0.9446883797645569\n",
      "execut\t0.9369127750396729\n",
      "influenc\t0.9365759491920471\n",
      "these\t0.935886025428772\n",
      "acquir\t0.9346303939819336\n",
      "avail\t0.9346036314964294\n",
      "-----------\n",
      "defin 531\n",
      "defin\t1.0\n",
      "determin\t0.9600980281829834\n",
      "describ\t0.9583099484443665\n",
      "paid\t0.9555740356445312\n",
      "basi\t0.9452660083770752\n",
      "pay\t0.9452496767044067\n",
      "made\t0.9448978304862976\n",
      "receiv\t0.9370494484901428\n",
      "A\t0.9347935318946838\n",
      "without\t0.9335318207740784\n",
      "prior\t0.9332242608070374\n",
      "period\t0.9288978576660156\n",
      "purpos\t0.9285261631011963\n",
      "except\t0.9268601536750793\n",
      "qualifi\t0.9255048632621765\n",
      "-----------\n",
      "israel 1946\n",
      "israel\t1.0\n",
      "purifi\t0.9144485592842102\n",
      "degrad\t0.9143059253692627\n",
      "particl\t0.9032479524612427\n",
      "batch\t0.8948826789855957\n",
      "SL\t0.8932253122329712\n",
      "isra\t0.8926929831504822\n",
      "neuron\t0.8905797600746155\n",
      "synthet\t0.890262246131897\n",
      "resist\t0.8848442435264587\n",
      "infect\t0.8832101821899414\n",
      "plc\t0.8818033337593079\n",
      "astaxanthin\t0.8798282742500305\n",
      "thick\t0.8768461346626282\n",
      "proof\t0.8760470747947693\n",
      "-----------\n",
      "book 1195\n",
      "book\t1.0\n",
      "agent\t0.8904727101325989\n",
      "document\t0.8106421232223511\n",
      "C\t0.8036602735519409\n",
      "whole\t0.7849509119987488\n",
      "exhibit\t0.7577553391456604\n",
      "represent\t0.7431110739707947\n",
      "assign\t0.7354387640953064\n",
      "each\t0.7310888767242432\n",
      "administr\t0.7270675897598267\n",
      "2022\t0.712823748588562\n",
      "rise\t0.7113714814186096\n",
      "immedi\t0.6983012557029724\n",
      "mean\t0.695709228515625\n",
      "percentag\t0.6901280283927917\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Look at some words\n",
    "# manual KNN\n",
    "from nltk.stem import PorterStemmer\n",
    "PORTER_STEMMER = PorterStemmer()\n",
    "\n",
    "# Set of words used in the original paper\n",
    "candidates = [\"weapons\", \"medical\", \"companies\", \"define\", \"israel\", \"book\"]\n",
    "\n",
    "for candidate in candidates:\n",
    "    test_word = PORTER_STEMMER.stem(candidate)\n",
    "    idx = vocab.stoi[test_word]\n",
    "    print(test_word, idx)\n",
    "\n",
    "    # Show top 10 most similar (based on cosine distance)\n",
    "    sims = F.cosine_similarity(weights[idx].unsqueeze(0), weights)\n",
    "    sim_vals, sim_idxs = torch.topk(sims, 15)\n",
    "\n",
    "    # Show ith nearest word and its score.\n",
    "    for i, v in zip(sim_idxs, sim_vals):\n",
    "        print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "    \n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "iiot\t1.5719436407089233\n",
      "mifid\t1.5622806549072266\n",
      "sef\t1.552869439125061\n",
      "sdk\t1.5519018173217773\n",
      "ria\t1.4965932369232178\n",
      "modem\t1.4920049905776978\n",
      "guidewir\t1.4848800897598267\n",
      "interbodi\t1.4842369556427002\n",
      "cgm\t1.4779086112976074\n",
      "ghz\t1.474395990371704\n",
      "dexcom\t1.469714879989624\n",
      "plane\t1.4693533182144165\n",
      "clia\t1.4622814655303955\n",
      "interoper\t1.4570252895355225\n",
      "mbp\t1.453446626663208\n",
      "iot\t1.4483671188354492\n",
      "peek\t1.4462167024612427\n",
      "voip\t1.4388659000396729\n",
      "exoskeleton\t1.4301104545593262\n",
      "m2m\t1.423481822013855\n",
      "cpt\t1.4216123819351196\n",
      "cellfx\t1.420039415359497\n",
      "mridium\t1.4180893898010254\n",
      "bluetooth\t1.4171274900436401\n",
      "excim\t1.412894368171692\n",
      "saa\t1.408990502357483\n",
      "ott\t1.4067158699035645\n",
      "router\t1.4054046869277954\n",
      "oracl\t1.400571346282959\n",
      "labview\t1.3922574520111084\n",
      "------------\n",
      "Topic 2\n",
      "biotherapeut\t2.1294736862182617\n",
      "cd20\t2.0815372467041016\n",
      "dendrit\t2.0779407024383545\n",
      "lymphoblast\t2.07721209526062\n",
      "linker\t2.0732641220092773\n",
      "antigen\t2.061290979385376\n",
      "cellecti\t2.0539138317108154\n",
      "cd19\t2.0537850856781006\n",
      "microenviron\t2.0336287021636963\n",
      "cd8\t2.0333425998687744\n",
      "scfv\t2.0253360271453857\n",
      "hnscc\t2.011608362197876\n",
      "cd16\t2.010772943496704\n",
      "atezolizumab\t2.0087428092956543\n",
      "lymphodeplet\t2.0056545734405518\n",
      "tme\t1.9910110235214233\n",
      "nhl\t1.9804670810699463\n",
      "cleav\t1.9722771644592285\n",
      "immunotherapeut\t1.9704560041427612\n",
      "epitop\t1.9659448862075806\n",
      "nivolumab\t1.9632055759429932\n",
      "transduct\t1.9619171619415283\n",
      "ipilimumab\t1.9599882364273071\n",
      "MM\t1.9539055824279785\n",
      "checkpoint\t1.9483311176300049\n",
      "hpv\t1.9440680742263794\n",
      "cd123\t1.9337750673294067\n",
      "reprogram\t1.9316176176071167\n",
      "immunotherapi\t1.9298921823501587\n",
      "trastuzumab\t1.9284745454788208\n",
      "------------\n",
      "Topic 3\n",
      "opa\t1.6599183082580566\n",
      "bwm\t1.612304449081421\n",
      "flowback\t1.5178247690200806\n",
      "mboe\t1.486673355102539\n",
      "ballast\t1.4766358137130737\n",
      "boem\t1.4300159215927124\n",
      "bunker\t1.4191982746124268\n",
      "tanker\t1.4170281887054443\n",
      "wotu\t1.4126834869384766\n",
      "uscg\t1.4098960161209106\n",
      "bsee\t1.408047080039978\n",
      "sdwa\t1.4047712087631226\n",
      "marpol\t1.4045482873916626\n",
      "valorem\t1.3798621892929077\n",
      "undril\t1.3773530721664429\n",
      "saltwat\t1.3545193672180176\n",
      "griffin\t1.3545054197311401\n",
      "eca\t1.354276418685913\n",
      "runoff\t1.3478548526763916\n",
      "migratori\t1.3435145616531372\n",
      "503b\t1.341966152191162\n",
      "uh\t1.3375784158706665\n",
      "pud\t1.335706114768982\n",
      "mbbl\t1.3326716423034668\n",
      "dra\t1.326898455619812\n",
      "cwa\t1.315472960472107\n",
      "boaz\t1.31472647190094\n",
      "investigatori\t1.308401346206665\n",
      "sabin\t1.3050532341003418\n",
      "vent\t1.30241060256958\n",
      "------------\n",
      "Topic 4\n",
      "poker\t1.347263216972351\n",
      "loung\t1.3277759552001953\n",
      "tournament\t1.3224838972091675\n",
      "quadrenni\t1.2719007730484009\n",
      "racetrack\t1.261754035949707\n",
      "auditorium\t1.2494909763336182\n",
      "grit\t1.2413158416748047\n",
      "itinerari\t1.2247318029403687\n",
      "atlanti\t1.2224221229553223\n",
      "wager\t1.220760703086853\n",
      "gaucho\t1.2109720706939697\n",
      "simulcast\t1.2082267999649048\n",
      "disney\t1.1691007614135742\n",
      "esport\t1.1546885967254639\n",
      "jsa\t1.1546310186386108\n",
      "lotteri\t1.1532249450683594\n",
      "vega\t1.152228593826294\n",
      "ancient\t1.1510725021362305\n",
      "sola\t1.1508102416992188\n",
      "hollywood\t1.1380727291107178\n",
      "sesam\t1.1377819776535034\n",
      "exhibitor\t1.1348085403442383\n",
      "wpt\t1.12886643409729\n",
      "jackpot\t1.1256704330444336\n",
      "reclin\t1.1174321174621582\n",
      "casino\t1.113993763923645\n",
      "guest\t1.1120104789733887\n",
      "tour\t1.1096183061599731\n",
      "eca\t1.107289433479309\n",
      "cabin\t1.1061737537384033\n",
      "------------\n",
      "Topic 5\n",
      "undril\t1.6702865362167358\n",
      "btu\t1.6655642986297607\n",
      "valorem\t1.6195570230484009\n",
      "geologist\t1.5784837007522583\n",
      "mcf\t1.5751217603683472\n",
      "mmcf\t1.5711402893066406\n",
      "recomplet\t1.5280773639678955\n",
      "geoscienc\t1.5139307975769043\n",
      "naaq\t1.4968355894088745\n",
      "mbbl\t1.4857828617095947\n",
      "wotu\t1.4831515550613403\n",
      "stratigraph\t1.4726508855819702\n",
      "bbl\t1.4352331161499023\n",
      "fpso\t1.4242832660675049\n",
      "nga\t1.4067615270614624\n",
      "mmbtu\t1.4062641859054565\n",
      "tmst\t1.3930394649505615\n",
      "arithmet\t1.392238736152649\n",
      "nsp\t1.3881863355636597\n",
      "ozon\t1.3877397775650024\n",
      "mmbbl\t1.3819870948791504\n",
      "grantor\t1.3805718421936035\n",
      "posco\t1.3701709508895874\n",
      "sdwa\t1.3697044849395752\n",
      "brine\t1.3639183044433594\n",
      "slag\t1.3630508184432983\n",
      "mboe\t1.354785680770874\n",
      "opec\t1.3511983156204224\n",
      "ethan\t1.3505041599273682\n",
      "transcanada\t1.3502219915390015\n",
      "------------\n",
      "Topic 6\n",
      "venat\t1.8512324094772339\n",
      "opac\t1.8139585256576538\n",
      "chlorin\t1.7795213460922241\n",
      "feedstock\t1.7542225122451782\n",
      "tio2\t1.730794072151184\n",
      "slurri\t1.701994776725769\n",
      "slag\t1.6672694683074951\n",
      "sportswear\t1.6503112316131592\n",
      "furnac\t1.6475830078125\n",
      "ilmenit\t1.6292016506195068\n",
      "uncoat\t1.6118656396865845\n",
      "brass\t1.6082466840744019\n",
      "lamin\t1.6044493913650513\n",
      "rutil\t1.5959609746932983\n",
      "stainless\t1.5890289545059204\n",
      "hardwood\t1.5868836641311646\n",
      "polyest\t1.586769700050354\n",
      "textil\t1.5857532024383545\n",
      "remanufactur\t1.568974494934082\n",
      "mdi\t1.5580812692642212\n",
      "resin\t1.5571377277374268\n",
      "vinyl\t1.5474772453308105\n",
      "polyol\t1.546533465385437\n",
      "grandevo\t1.5446257591247559\n",
      "maleic\t1.5439059734344482\n",
      "compx\t1.5384398698806763\n",
      "fiberglass\t1.5350981950759888\n",
      "mill\t1.5284982919692993\n",
      "butadien\t1.5279897451400757\n",
      "softwood\t1.5230295658111572\n",
      "------------\n",
      "Topic 7\n",
      "ginni\t2.067721128463745\n",
      "otti\t2.003737688064575\n",
      "cmb\t1.998551607131958\n",
      "freddi\t1.9873045682907104\n",
      "domiciliari\t1.9692351818084717\n",
      "af\t1.9421273469924927\n",
      "workout\t1.8789126873016357\n",
      "fha\t1.8750150203704834\n",
      "537\t1.8715035915374756\n",
      "unobserv\t1.866047739982605\n",
      "divorc\t1.8633641004562378\n",
      "tdr\t1.8615925312042236\n",
      "misfeas\t1.8532068729400635\n",
      "jumbo\t1.8519842624664307\n",
      "substandard\t1.844917893409729\n",
      "ibnr\t1.8413894176483154\n",
      "fitch\t1.835525631904602\n",
      "unitranch\t1.8229697942733765\n",
      "fhlbi\t1.8225326538085938\n",
      "subchapt\t1.809617519378662\n",
      "qtl\t1.8076863288879395\n",
      "gse\t1.803865909576416\n",
      "reo\t1.7989088296890259\n",
      "lae\t1.79291570186615\n",
      "hail\t1.7877110242843628\n",
      "subprim\t1.7859477996826172\n",
      "nrsro\t1.7843894958496094\n",
      "fanni\t1.776654601097107\n",
      "dsi\t1.7749384641647339\n",
      "mae\t1.7672446966171265\n",
      "------------\n",
      "Topic 8\n",
      "fdi\t1.6600651741027832\n",
      "fdia\t1.65761399269104\n",
      "glb\t1.6306639909744263\n",
      "cblr\t1.5949710607528687\n",
      "fdicia\t1.5848244428634644\n",
      "dif\t1.5546975135803223\n",
      "nonbank\t1.5343514680862427\n",
      "countercycl\t1.5037424564361572\n",
      "bhca\t1.4756540060043335\n",
      "cet1\t1.4729549884796143\n",
      "hvcre\t1.4679145812988281\n",
      "lcr\t1.4659401178359985\n",
      "dfpi\t1.463646411895752\n",
      "egrrcpa\t1.4549347162246704\n",
      "1956\t1.4455041885375977\n",
      "unsound\t1.4449816942214966\n",
      "volcker\t1.436270833015442\n",
      "cra\t1.4327805042266846\n",
      "nsfr\t1.429992437362671\n",
      "23a\t1.4288485050201416\n",
      "23b\t1.4189205169677734\n",
      "nonaffili\t1.4188276529312134\n",
      "depositor\t1.4040489196777344\n",
      "basel\t1.3943649530410767\n",
      "undercapit\t1.3721143007278442\n",
      "interag\t1.3258368968963623\n",
      "teller\t1.3227107524871826\n",
      "qnb\t1.3191847801208496\n",
      "unimpair\t1.3185445070266724\n",
      "peoplesbank\t1.309520959854126\n",
      "------------\n",
      "Topic 9\n",
      "comcast\t1.5338047742843628\n",
      "ott\t1.4977467060089111\n",
      "ilec\t1.4902607202529907\n",
      "retransmiss\t1.4883575439453125\n",
      "fcc\t1.4846136569976807\n",
      "jsa\t1.4481275081634521\n",
      "mvpd\t1.4231911897659302\n",
      "nbc\t1.4032832384109497\n",
      "mbp\t1.3936715126037598\n",
      "ovd\t1.3852280378341675\n",
      "broadband\t1.3548628091812134\n",
      "geostationari\t1.3540138006210327\n",
      "gci\t1.3483847379684448\n",
      "hbo\t1.3481279611587524\n",
      "quadrenni\t1.3373761177062988\n",
      "ibew\t1.3299449682235718\n",
      "mhz\t1.3247196674346924\n",
      "opent\t1.3130100965499878\n",
      "dma\t1.3028249740600586\n",
      "carriag\t1.2972707748413086\n",
      "mntv\t1.2950409650802612\n",
      "fmcsa\t1.2740778923034668\n",
      "lte\t1.2679108381271362\n",
      "itu\t1.2662605047225952\n",
      "lsa\t1.2445034980773926\n",
      "ransomwar\t1.2414145469665527\n",
      "lendingtre\t1.2386713027954102\n",
      "equifax\t1.233044981956482\n",
      "snf\t1.2271677255630493\n",
      "courttv\t1.224826455116272\n",
      "------------\n",
      "Topic 10\n",
      "revascular\t1.803455114364624\n",
      "ide\t1.79365873336792\n",
      "inpati\t1.75498628616333\n",
      "anatomi\t1.739067792892456\n",
      "cpt\t1.7334058284759521\n",
      "amput\t1.7117832899093628\n",
      "cardiologist\t1.7033143043518066\n",
      "porcin\t1.6643061637878418\n",
      "allograft\t1.6587872505187988\n",
      "graft\t1.6523826122283936\n",
      "surgeri\t1.6408100128173828\n",
      "coronari\t1.6328973770141602\n",
      "thorac\t1.6320284605026245\n",
      "distal\t1.6213901042938232\n",
      "surgeon\t1.6189817190170288\n",
      "arteri\t1.6161047220230103\n",
      "comorbid\t1.615116000175476\n",
      "biocompat\t1.6114848852157593\n",
      "mdsap\t1.60741126537323\n",
      "bovin\t1.6057556867599487\n",
      "occlus\t1.5995702743530273\n",
      "outpati\t1.5923247337341309\n",
      "multicent\t1.5919753313064575\n",
      "endovascular\t1.5902423858642578\n",
      "cathet\t1.5881013870239258\n",
      "anatom\t1.5862348079681396\n",
      "interbodi\t1.585706114768982\n",
      "premarket\t1.5851892232894897\n",
      "cortic\t1.5833810567855835\n",
      "arrhythmia\t1.583166480064392\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Look at most similar words per topic vector.\n",
    "\n",
    "V, K = weights.size()\n",
    "for i in range(K):\n",
    "    print(f\"Topic {i+1}\")\n",
    "    vals, idxs = torch.topk(torch.abs(weights[:, i]), 30)\n",
    "    for i, v in zip(idxs, vals):\n",
    "        print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "MODELSAVE_PATH = \"./modelsaves\"\n",
    "\n",
    "models_k = dict()\n",
    "\n",
    "for filename in os.listdir(MODELSAVE_PATH):\n",
    "    \n",
    "    num_topics = filename.split(\"_\")[1][1:]\n",
    "    num_topics = int(num_topics)\n",
    "    \n",
    "    model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODELSAVE_PATH, filename)))\n",
    "    model.eval()\n",
    "    models_k[num_topics] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(model):\n",
    "    PORTER_STEMMER = PorterStemmer()\n",
    "    # Set of words used in the original paper\n",
    "    candidates = [\"weapons\", \"medical\", \"companies\", \"define\", \"israel\", \"book\"]\n",
    "    \n",
    "    # Extract the vocab-topic matrix (known as R in the paper).\n",
    "    # It has dimensions |V| x K: vocab size x number of topics\n",
    "    decoder = model.decoder[0]\n",
    "    weights = decoder.weight.data.detach().clone()\n",
    "\n",
    "    for candidate in candidates:\n",
    "        test_word = PORTER_STEMMER.stem(candidate)\n",
    "        idx = vocab.stoi[test_word]\n",
    "        print(test_word, idx)\n",
    "\n",
    "        # Show top 10 most similar (based on cosine distance)\n",
    "        sims = F.cosine_similarity(weights[idx].unsqueeze(0), weights)\n",
    "        sim_vals, sim_idxs = torch.topk(sims, 15)\n",
    "\n",
    "        # Show ith nearest word and its score.\n",
    "        for i, v in zip(sim_idxs, sim_vals):\n",
    "            print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "\n",
    "        print(\"-----------\")\n",
    "    \n",
    "    V, K = weights.size()\n",
    "    for i in range(K):\n",
    "        print(f\"Topic {i+1}\")\n",
    "        vals, idxs = torch.topk(torch.abs(weights[:, i]), 30)\n",
    "        for i, v in zip(idxs, vals):\n",
    "            print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weapon 6165\n",
      "weapon\t1.0\n",
      "headset\t0.9978045225143433\n",
      "diod\t0.9977712631225586\n",
      "fpga\t0.9975942969322205\n",
      "upcom\t0.9969573020935059\n",
      "microwav\t0.9967873692512512\n",
      "ergonom\t0.9967591762542725\n",
      "microdisplay\t0.9961642026901245\n",
      "ubiquitor\t0.995185375213623\n",
      "microprocessor\t0.9949306845664978\n",
      "avx\t0.9949303269386292\n",
      "waveguid\t0.9942797422409058\n",
      "ip\t0.9927443861961365\n",
      "sonar\t0.99266117811203\n",
      "labview\t0.9923931956291199\n",
      "-----------\n",
      "medic 235\n",
      "medic\t1.0\n",
      "univers\t0.9583813548088074\n",
      "devic\t0.9522219300270081\n",
      "rapid\t0.9455239176750183\n",
      "inc\t0.9404319524765015\n",
      "care\t0.9309444427490234\n",
      "hospit\t0.9130054712295532\n",
      "promot\t0.9118832349777222\n",
      "behavior\t0.9088248014450073\n",
      "version\t0.9068402647972107\n",
      "healthcar\t0.8899935483932495\n",
      "rapidli\t0.8878748416900635\n",
      "foundat\t0.8871327042579651\n",
      "still\t0.8849836587905884\n",
      "patient\t0.8839438557624817\n",
      "-----------\n",
      "compani 3\n",
      "compani\t1.0\n",
      "entir\t0.9895139932632446\n",
      "corpor\t0.9875907897949219\n",
      "maintain\t0.9875349998474121\n",
      "collect\t0.9873965382575989\n",
      "adopt\t0.9861133694648743\n",
      "As\t0.9843692183494568\n",
      "influenc\t0.9832512140274048\n",
      "abil\t0.9816815853118896\n",
      "growth\t0.9792988300323486\n",
      "offic\t0.9781963229179382\n",
      "sourc\t0.9776283502578735\n",
      "meet\t0.9769752025604248\n",
      "acquir\t0.9763994812965393\n",
      "factor\t0.9759693741798401\n",
      "-----------\n",
      "defin 531\n",
      "defin\t1.0\n",
      "made\t0.9977492690086365\n",
      "except\t0.9961791634559631\n",
      "determin\t0.9945410490036011\n",
      "such\t0.992107093334198\n",
      "taken\t0.9884191155433655\n",
      "pay\t0.9858917593955994\n",
      "provis\t0.9858624935150146\n",
      "manner\t0.9854063391685486\n",
      "prior\t0.9840171933174133\n",
      "circumst\t0.9835441708564758\n",
      "limit\t0.980140745639801\n",
      "issu\t0.9786840081214905\n",
      "would\t0.9785784482955933\n",
      "upon\t0.9785692691802979\n",
      "-----------\n",
      "israel 1946\n",
      "israel\t1.0\n",
      "batch\t0.9958117604255676\n",
      "sequenc\t0.9955333471298218\n",
      "dispers\t0.9948574304580688\n",
      "nucleic\t0.9932422041893005\n",
      "viru\t0.9870507717132568\n",
      "viral\t0.9852288365364075\n",
      "greatli\t0.9851056933403015\n",
      "dna\t0.9806572794914246\n",
      "rna\t0.9804084300994873\n",
      "recombin\t0.97967928647995\n",
      "pathogen\t0.978667676448822\n",
      "resist\t0.9775886535644531\n",
      "protein\t0.9758047461509705\n",
      "bayer\t0.974865734577179\n",
      "-----------\n",
      "book 1195\n",
      "book\t1.0\n",
      "account\t0.991607666015625\n",
      "consider\t0.9914016127586365\n",
      "stock\t0.989940345287323\n",
      "fee\t0.9898459911346436\n",
      "contractu\t0.9864426255226135\n",
      "such\t0.9845533967018127\n",
      "manner\t0.9835518002510071\n",
      "aggreg\t0.9834418296813965\n",
      "extent\t0.9810120463371277\n",
      "common\t0.9788485169410706\n",
      "would\t0.9781138896942139\n",
      "made\t0.9774724245071411\n",
      "purpos\t0.9757786393165588\n",
      "taken\t0.9754866361618042\n",
      "-----------\n",
      "Topic 1\n",
      "venat\t2.7896182537078857\n",
      "sdwa\t2.7078487873077393\n",
      "feedstock\t2.6734540462493896\n",
      "downhol\t2.6715829372406006\n",
      "undril\t2.5957937240600586\n",
      "slag\t2.5907938480377197\n",
      "slurri\t2.590029716491699\n",
      "mbbl\t2.58734393119812\n",
      "migratori\t2.5851709842681885\n",
      "wotu\t2.556809186935425\n",
      "tsca\t2.5487561225891113\n",
      "butan\t2.5451362133026123\n",
      "biotherapeut\t2.5424067974090576\n",
      "endanger\t2.5257420539855957\n",
      "epoxi\t2.523458242416382\n",
      "reprogram\t2.4979639053344727\n",
      "mmbbl\t2.485645055770874\n",
      "geoscienc\t2.4803409576416016\n",
      "elicit\t2.461041212081909\n",
      "nga\t2.4599313735961914\n",
      "frac\t2.452641487121582\n",
      "wellbor\t2.4520556926727295\n",
      "brine\t2.450670003890991\n",
      "wellsit\t2.444349527359009\n",
      "chlorid\t2.438411235809326\n",
      "amin\t2.433640241622925\n",
      "mboe\t2.422307014465332\n",
      "stratigraph\t2.4218249320983887\n",
      "nucleotid\t2.420255184173584\n",
      "recomplet\t2.4192049503326416\n",
      "------------\n",
      "Topic 2\n",
      "fcc\t2.6318395137786865\n",
      "carriag\t2.6302855014801025\n",
      "retransmiss\t2.6273913383483887\n",
      "broadcast\t2.4732656478881836\n",
      "marriott\t2.4230103492736816\n",
      "dma\t2.422990083694458\n",
      "mbp\t2.410048007965088\n",
      "viewer\t2.3972957134246826\n",
      "usf\t2.3905603885650635\n",
      "quadrenni\t2.389950752258301\n",
      "comcast\t2.372241258621216\n",
      "mvpd\t2.3557920455932617\n",
      "modem\t2.3481874465942383\n",
      "uhf\t2.318263053894043\n",
      "mhz\t2.315274238586426\n",
      "nbc\t2.305485486984253\n",
      "broadband\t2.3001365661621094\n",
      "voip\t2.2958476543426514\n",
      "televis\t2.290736675262451\n",
      "terrestri\t2.268950939178467\n",
      "revpar\t2.2686853408813477\n",
      "ott\t2.2632460594177246\n",
      "voi\t2.2565982341766357\n",
      "hoa\t2.2551968097686768\n",
      "ovd\t2.254793405532837\n",
      "itu\t2.253239393234253\n",
      "bandwidth\t2.2429466247558594\n",
      "lte\t2.2389206886291504\n",
      "inmarsat\t2.2375454902648926\n",
      "hotel\t2.236891031265259\n",
      "------------\n",
      "Topic 3\n",
      "cellecti\t2.519272565841675\n",
      "faculti\t2.5192477703094482\n",
      "insever\t2.5120906829833984\n",
      "smpc\t2.5007402896881104\n",
      "docetaxel\t2.4967591762542725\n",
      "postsecondari\t2.4870493412017822\n",
      "pretens\t2.4657034873962402\n",
      "optometrist\t2.442056655883789\n",
      "etasu\t2.391770601272583\n",
      "14e\t2.381681442260742\n",
      "asco\t2.3655943870544434\n",
      "recist\t2.3637359142303467\n",
      "dimer\t2.3566761016845703\n",
      "tricar\t2.3555214405059814\n",
      "nccn\t2.35017466545105\n",
      "monocyt\t2.322160005569458\n",
      "monogen\t2.3152856826782227\n",
      "aminotransferas\t2.3081374168395996\n",
      "indol\t2.3074288368225098\n",
      "qidp\t2.2979254722595215\n",
      "mcrpc\t2.289175510406494\n",
      "healthlynk\t2.288733959197998\n",
      "metastasi\t2.2764830589294434\n",
      "rld\t2.270219326019287\n",
      "hmo\t2.2695705890655518\n",
      "enrolle\t2.2674872875213623\n",
      "flexpath\t2.264474630355835\n",
      "adventiti\t2.2614712715148926\n",
      "imm\t2.259613513946533\n",
      "neutropenia\t2.256669759750366\n",
      "------------\n",
      "Topic 4\n",
      "endovascular\t2.584852695465088\n",
      "cardiologist\t2.3953299522399902\n",
      "fixat\t2.394341230392456\n",
      "interbodi\t2.374715566635132\n",
      "cpt\t2.360334634780884\n",
      "aneurysm\t2.3364412784576416\n",
      "mdsap\t2.3308358192443848\n",
      "allograft\t2.328652858734131\n",
      "graft\t2.318962812423706\n",
      "amput\t2.30810809135437\n",
      "trauma\t2.284390926361084\n",
      "anatomi\t2.2620387077331543\n",
      "ablat\t2.261657476425171\n",
      "femor\t2.241537570953369\n",
      "ide\t2.2399916648864746\n",
      "handpiec\t2.205014705657959\n",
      "revascular\t2.1986899375915527\n",
      "atherectomi\t2.1978940963745117\n",
      "exhibitor\t2.194091796875\n",
      "sutur\t2.1834499835968018\n",
      "VR\t2.1735150814056396\n",
      "amniot\t2.1632843017578125\n",
      "artifact\t2.1570756435394287\n",
      "surgeon\t2.1559810638427734\n",
      "bioresorb\t2.1527817249298096\n",
      "spine\t2.150890350341797\n",
      "guidewir\t2.145205020904541\n",
      "aorta\t2.13256573677063\n",
      "stent\t2.1302683353424072\n",
      "poker\t2.107814311981201\n",
      "------------\n",
      "Topic 5\n",
      "papuc\t2.230210781097412\n",
      "dcpsc\t2.225950002670288\n",
      "afudc\t2.1733627319335938\n",
      "njbpu\t2.167980194091797\n",
      "feja\t2.1400747299194336\n",
      "opeb\t2.1335346698760986\n",
      "retailcustom\t2.1190249919891357\n",
      "phisco\t2.112922191619873\n",
      "836\t2.109224557876587\n",
      "todecemb\t2.107210159301758\n",
      "tomarch\t2.098646640777588\n",
      "eddyston\t2.0859222412109375\n",
      "eima\t2.071605920791626\n",
      "carfr\t2.071305990219116\n",
      "zec\t2.0678296089172363\n",
      "mystic\t2.0665698051452637\n",
      "eroa\t2.062748908996582\n",
      "2046\t2.049612283706665\n",
      "totalretailcustom\t2.049318313598633\n",
      "kV\t2.0438477993011475\n",
      "dpsc\t2.041121006011963\n",
      "gruver\t2.0328025817871094\n",
      "794\t2.029351234436035\n",
      "cfd\t2.012132167816162\n",
      "mandatorili\t2.0118072032928467\n",
      "avsr\t2.0061299800872803\n",
      "546\t2.0012292861938477\n",
      "misfeas\t2.000930070877075\n",
      "ofcustom\t1.990805745124817\n",
      "mapp\t1.9897831678390503\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "analysis(models_k[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
