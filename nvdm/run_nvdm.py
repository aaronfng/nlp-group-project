"""
Command line version of notebook.
"""
#!/usr/bin/env python


import argparse
from timeit import default_timer as timer
from datetime import timedelta

from collections import Counter
from torchtext.vocab import Vocab
import torch
from torch.utils.data import Dataset, DataLoader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device {device}")

import data_handling as data
from models import nvdm

parser = argparse.ArgumentParser()
parser.add_argument("-k", type=int, dest="num_topics", required=True)
parser.add_argument("-e", type=int, dest="num_epochs", required=True)
args = parser.parse_args()
print(args)

print("Load data..")

# Data that has already been preprocessed
# Generated by applying pp.preprocess_text() to each BD,
# then saved to a TSV
DATA_CLEAN_PATH = "./data/bds_1_clean.txt"
IDs_raw, BDs_raw = data.load_raw(DATA_CLEAN_PATH)

# Some entries have empty BDs, so filter those out
IDs = []
BDs = []
for iid, bd in zip(IDs_raw, BDs_raw):
    if len(bd) > 0:
        IDs.append(iid)
        BDs.append(bd)

print(len(IDs), len(BDs))

# Build frequency table
# (cleaned data joins tokens by space)
counter = Counter()
for desc in BDs:
    counter.update(desc.split(" "))

# PyTorch torchtext vocabulary converts tokens to indices and vice versa.
# Also has an '<unk>' for OOV words (might be useful later).
vocab = Vocab(counter,
              max_size=10000,
              min_freq=1,
              specials=['<unk>'])
print(len(vocab))


class BDDataset(Dataset):
    """ Very simple dataset object. Stores all the passages.
    
    This is just for compatibility with PyTorch DataLoader.
    """
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]


# "Preprocessing" function: just splits the text
# The file's text is already preprocessed.
def text_pipeline(text):
    return [vocab[token] for token in text.split(" ")]


def collate_batch(batch):
    """ Convert a batch of text (each a list of tokens) into appropriate torch tensors.
    
    Modification of https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html.
    We don't need labels.
    """
    # Offsets tells the model (which will use EmbeddingBag) where each text starts.
    text_list, offsets = [], [0]
    for _text in batch:
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
        offsets.append(processed_text.size(0))

    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return text_list.to(device), offsets.to(device)

# Create data loader to iterate over dataset in batches during training/evaluation
dataset = BDDataset(BDs)
batch_size = 64
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)
hidden_size = 500
num_topics = args.num_topics

# Training setup

# Total number of epochs
outer_epochs = args.num_epochs

model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)
model = model.to(device)
model.train()

# Trains both the encoder and decoder at the same time.
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

start_time = timer()
print("Start training...")


for epoch in range(outer_epochs):

    loss_sum = 0.0
    rec_sum = 0.0
    kl_sum = 0.0
    n = len(data_loader)

    for idx, (text, offsets) in enumerate(data_loader):
        text = text.to(device)
        offsets = offsets.to(device)

        optimizer.zero_grad()
        logits, loss_dict = model(text, offsets, kl_weight=1.0)
        loss = loss_dict["total"]
        loss.backward()

        optimizer.step()

        # For printing
        loss_sum += loss.item()
        rec_sum += loss_dict["rec"].item()
        kl_sum += loss_dict["kl"].item()

    model_str = "All" # "Enc" if switch == 0 else "Dec"
    print(f"[Time: {timedelta(seconds=timer() - start_time)}, Epoch {epoch + 1}] Loss {loss_sum/n}, Rec {rec_sum/n}, KL {kl_sum/n}")

print("Save model...")
MODELSAVE_PATH = f"./modelsaves/nvdm_k{num_topics}_{outer_epochs}epochs.pt"
torch.save(model.state_dict(), MODELSAVE_PATH)
print("Done.")
