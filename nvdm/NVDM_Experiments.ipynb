{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_handling as data\n",
    "import preprocess as pp\n",
    "from models import nvdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original data\n",
    "# DATA_RAW_PATH = \"./data/bds_1.txt\"\n",
    "# IDs, BDs = data.load_raw(DATA_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that has already been preprocessed\n",
    "# Generated by applying pp.preprocess_text() to each BD,\n",
    "# then saved to a TSV\n",
    "DATA_CLEAN_PATH = \"./data/bds_1_clean.txt\"\n",
    "IDs_raw, BDs_raw = data.load_raw(DATA_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 2034\n"
     ]
    }
   ],
   "source": [
    "# Some entries have empty BDs, so filter those out\n",
    "IDs = []\n",
    "BDs = []\n",
    "for iid, bd in zip(IDs_raw, BDs_raw):\n",
    "    if len(bd) > 0:\n",
    "        IDs.append(iid)\n",
    "        BDs.append(bd)\n",
    "\n",
    "print(len(IDs), len(BDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following PyTorch's tutorial for data setup.\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build frequency table\n",
    "# (cleaned data joins tokens by space)\n",
    "# counter = Counter()\n",
    "# for desc in BDs:\n",
    "#     counter.update(desc.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "# PyTorch torchtext vocabulary converts tokens to indices and vice versa.\n",
    "# Also has an '<unk>' for OOV words (might be useful later).\n",
    "# vocab = Vocab(counter,\n",
    "#               max_size=10000,\n",
    "#               min_freq=1,\n",
    "#               specials=['<unk>'])\n",
    "# print(len(vocab))\n",
    "# actual is 70770 without max_size restriction\n",
    "\n",
    "# Save the vocab to file\n",
    "# with open(\"./vocabs/vocab_bds_1_clean_10000.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(vocab, f)\n",
    "\n",
    "# Load the vocab from file\n",
    "with open(\"./vocabs/vocab_bds_1_clean_10000.pickle\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDataset(Dataset):\n",
    "    \"\"\" Very simple dataset object. Stores all the passages.\n",
    "    \n",
    "    This is just for compatibility with PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Preprocessing\" function: just splits the text\n",
    "# The file's text is already preprocessed.\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in text.split(\" \")]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Convert a batch of text (each a list of tokens) into appropriate torch tensors.\n",
    "    \n",
    "    Modification of https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html.\n",
    "    We don't need labels.\n",
    "    \"\"\"\n",
    "    # Offsets tells the model (which will use EmbeddingBag) where each text starts.\n",
    "    text_list, offsets = [], [0]\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader to iterate over dataset in batches during training/evaluation\n",
    "dataset = BDDataset(BDs)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "hidden_size = 500\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time: 0:00:07.077526, Epoch 1] Loss 3533693.89453125, Rec 3532094.71875, KL 1599.213567018509\n",
      "[Time: 0:00:13.945147, Epoch 2] Loss 3339778.41796875, Rec 3337470.7890625, KL 2307.6848335266113\n",
      "[Time: 0:00:20.647853, Epoch 3] Loss 3054039.77734375, Rec 3050626.8125, KL 3412.9466705322266\n",
      "[Time: 0:00:27.374750, Epoch 4] Loss 2900912.69921875, Rec 2896842.7109375, KL 4070.023857116699\n",
      "[Time: 0:00:34.038899, Epoch 5] Loss 2848929.4765625, Rec 2844505.6953125, KL 4423.774795532227\n",
      "[Time: 0:00:40.745445, Epoch 6] Loss 2833701.453125, Rec 2829115.0859375, KL 4586.375625610352\n",
      "[Time: 0:00:47.944588, Epoch 7] Loss 2828658.16796875, Rec 2824032.140625, KL 4625.994033813477\n",
      "[Time: 0:00:54.914819, Epoch 8] Loss 2825123.140625, Rec 2820501.44921875, KL 4621.645446777344\n",
      "[Time: 0:01:01.644543, Epoch 9] Loss 2822650.6875, Rec 2818091.40625, KL 4559.269622802734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ada675e97af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_UCL_MSc/NLP_COMP0087/venv-comp0087/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_UCL_MSc/NLP_COMP0087/venv-comp0087/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_UCL_MSc/NLP_COMP0087/venv-comp0087/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c4b064acde10>\u001b[0m in \u001b[0;36mcollate_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtext_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c4b064acde10>\u001b[0m in \u001b[0;36mtext_pipeline\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The file's text is already preprocessed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c4b064acde10>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The file's text is already preprocessed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_UCL_MSc/NLP_COMP0087/venv-comp0087/lib/python3.9/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "\n",
    "# Total number of epochs\n",
    "outer_epochs = 200\n",
    "\n",
    "# Epochs for training the encoder/decoder on each alternation.\n",
    "inner_epochs = 1\n",
    "\n",
    "model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Trains both the encoder and decoder at the same time.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "for epoch in range(outer_epochs):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    rec_sum = 0.0\n",
    "    kl_sum = 0.0\n",
    "    n = len(data_loader)\n",
    "\n",
    "    for idx, (text, offsets) in enumerate(data_loader):\n",
    "        text = text.to(device)\n",
    "        offsets = offsets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(text, offsets, kl_weight=1.0)\n",
    "        loss = loss_dict[\"total\"].sum()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # For printing\n",
    "        loss_sum += loss.item()\n",
    "        rec_sum += loss_dict[\"rec\"].sum().item()\n",
    "        kl_sum += loss_dict[\"kl\"].sum().item()\n",
    "\n",
    "    model_str = \"All\" # \"Enc\" if switch == 0 else \"Dec\"\n",
    "    print(f\"[Time: {timedelta(seconds=timer() - start_time)}, Epoch {epoch + 1}] Loss {loss_sum/n}, Rec {rec_sum/n}, KL {kl_sum/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "- The original paper alternates between the encoder and decoder when training. i.e. It trains the decoder first for some (e.g. 10) iterations, fixing the encoders parameters. Then it trains the encoder, fixing the decoder's parameters. This is one epoch, which is repeated some number of times until convergence. However, this results in poor training performance: the KL is observed to fluctuate. The encoder and decoder are unable to jointly converge. By training them all together both the reconstruction loss and KL appear to go down.\n",
    "- Right now we weight the reconstruction and KL losses equally: $L_{total} = L_{rec} + L_{KL}$. We could define a hyperparameter $\\beta$ so that $L_{total} = L_{rec} + \\beta L_{KL}$, which might help balance the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELSAVE_PATH = \"./modelsaves/nvdm_k10_300epochs.pt\"\n",
    "# torch.save(model.state_dict(), MODELSAVE_PATH)\n",
    "\n",
    "model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "model.load_state_dict(torch.load(MODELSAVE_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocab-topic matrix (known as R in the paper).\n",
    "# It has dimensions |V| x K: vocab size x number of topics\n",
    "decoder = model.decoder[0]\n",
    "weights = decoder.weight.data.detach().clone()\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some words\n",
    "# manual KNN\n",
    "from nltk.stem import PorterStemmer\n",
    "PORTER_STEMMER = PorterStemmer()\n",
    "\n",
    "# Set of words used in the original paper\n",
    "candidates = [\"weapons\", \"medical\", \"companies\", \"define\", \"israel\", \"book\"]\n",
    "\n",
    "for candidate in candidates:\n",
    "    test_word = PORTER_STEMMER.stem(candidate)\n",
    "    idx = vocab.stoi[test_word]\n",
    "    print(test_word, idx)\n",
    "\n",
    "    # Show top 10 most similar (based on cosine distance)\n",
    "    sims = F.cosine_similarity(weights[idx].unsqueeze(0), weights)\n",
    "    sim_vals, sim_idxs = torch.topk(sims, 15)\n",
    "\n",
    "    # Show ith nearest word and its score.\n",
    "    for i, v in zip(sim_idxs, sim_vals):\n",
    "        print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "    \n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at most similar words per topic vector.\n",
    "\n",
    "V, K = weights.size()\n",
    "for i in range(K):\n",
    "    print(f\"Topic {i+1}\")\n",
    "    vals, idxs = torch.topk(torch.abs(weights[:, i]), 30)\n",
    "    for i, v in zip(idxs, vals):\n",
    "        print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(model):\n",
    "    \"\"\" Qualitative analysis of topic model. \"\"\"\n",
    "    \n",
    "    PORTER_STEMMER = PorterStemmer()\n",
    "    # Set of words used in the original NVDM paper\n",
    "    candidates = [\"weapons\", \"medical\", \"companies\", \"define\", \"israel\", \"book\"]\n",
    "    \n",
    "    # Extract the vocab-topic matrix (known as R in the paper).\n",
    "    # It has dimensions |V| x K: vocab size x number of topics\n",
    "    decoder = model.decoder[0]\n",
    "    weights = decoder.weight.data.detach().clone()\n",
    "\n",
    "    for candidate in candidates:\n",
    "        test_word = PORTER_STEMMER.stem(candidate)\n",
    "        idx = vocab.stoi[test_word]\n",
    "        print(test_word, idx)\n",
    "\n",
    "        # Show top 10 most similar (based on cosine distance)\n",
    "        sims = F.cosine_similarity(weights[idx].unsqueeze(0), weights)\n",
    "        sim_vals, sim_idxs = torch.topk(sims, 15)\n",
    "\n",
    "        # Show ith nearest word and its score.\n",
    "        for i, v in zip(sim_idxs, sim_vals):\n",
    "            print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "\n",
    "        print(\"-----------\")\n",
    "    \n",
    "    V, K = weights.size()\n",
    "    for i in range(K):\n",
    "        print(f\"Topic {i+1}\")\n",
    "        vals, idxs = torch.topk(torch.abs(weights[:, i]), 30)\n",
    "        for i, v in zip(idxs, vals):\n",
    "            print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic coherence.\n",
    "def umass_score(tf):\n",
    "    \"\"\" Compute topic coherence using UMass metric.\n",
    "    \n",
    "    Ref: http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "    \n",
    "    tf: term-frequency matrix for each document.\n",
    "        Each i^th row is the BOW representation of the i^th document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # D(wi): count of documents containing the word wi (i.e. df)\n",
    "    Dwi = np.array(np.sum(tf > 0, axis=0))[0]\n",
    "\n",
    "    W_bin = np.zeros_like(tf)\n",
    "    W_bin[tf > 0] = 1\n",
    "    \n",
    "    # D(wi, wj): count of documents containing both words wi and wj\n",
    "    Dwi_wj = W_bin.T @ W_bin\n",
    "\n",
    "    score_umass = np.log((Dwi_wj + 1)/ Dwi)\n",
    "    \n",
    "    return score_umass\n",
    "\n",
    "\n",
    "def topic_coherence(topic_vocab, n_top_words, pair_score):\n",
    "    \"\"\"\n",
    "    topic_vocab: dimensions (number of topics, vocabulary size).\n",
    "    model.components_ for LDA, and the \"semantic embedding\" matrix in the decoder for NVDM.\n",
    "    \n",
    "    pair_score: matrix of scores (e.g. UMass)\n",
    "    \"\"\"\n",
    "    coherences = []\n",
    "    for topic_idx, topic in enumerate(topic_vocab):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        coh = 0\n",
    "        for i in range(len(top_features_ind)):\n",
    "            for j in range(i):\n",
    "                coh += pair_score[top_features_ind[i], top_features_ind[j]]\n",
    "        coherences.append(coh)\n",
    "    return coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "MODELSAVE_PATH = \"./modelsaves\"\n",
    "\n",
    "models_k = dict()\n",
    "k_values = []\n",
    "\n",
    "for filename in os.listdir(MODELSAVE_PATH):\n",
    "    \n",
    "    num_topics = filename.split(\"_\")[1][1:]\n",
    "    num_topics = int(num_topics)\n",
    "    k_values.append(num_topics)\n",
    "    \n",
    "    model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, \"cpu\")\n",
    "    model.load_state_dict(torch.load(os.path.join(MODELSAVE_PATH, filename), map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    models_k[num_topics] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the BOW matrix manually, using the existing Vocab's token-to-index mapping.\n",
    "bow_mat = np.zeros((len(BDs), len(vocab)))\n",
    "for d, bd in enumerate(BDs):\n",
    "    token_idxs = vocab.lookup_indices(bd.split(\" \"))\n",
    "    word_counts = Counter(token_idxs)\n",
    "    for w, count in word_counts.items():\n",
    "        bow_mat[d, w] = count   \n",
    "bow_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_umass_mat = umass_score(bow_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values.sort()\n",
    "tc_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f'Running k = {k}')\n",
    "    this_model = models_k[k]\n",
    "    \n",
    "    # Extract the topic vocab matrix\n",
    "    decoder = this_model.decoder[0]\n",
    "    weights = decoder.weight.data.detach().clone().numpy()\n",
    "    topic_vocab_mat = weights.T\n",
    "    coherences = topic_coherence(topic_vocab_mat, 10, score_umass_mat)\n",
    "    this_c = np.median(coherences)\n",
    "\n",
    "    tc_values.append(this_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, tc_values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(topic_vocab, feature_names, n_top_words, title):\n",
    "    K = len(topic_vocab)\n",
    "    n_x = 5\n",
    "    n_y = int(np.ceil(K / n_x))\n",
    "    fig, axes = plt.subplots(n_y, n_x, figsize=(2.5 * n_x, 4 * n_y), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(topic_vocab):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 14})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [vocab.itos[i] for i in range(0, len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "this_model = models_k[k]\n",
    "decoder = this_model.decoder[0]\n",
    "weights = decoder.weight.data.detach().clone().numpy()\n",
    "topic_vocab_mat = weights.T\n",
    "\n",
    "plot_top_words(topic_vocab_mat, feature_names, 10, f\"NVDM K={k} Topics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
