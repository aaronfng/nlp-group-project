{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from edgar import Company\n",
    "import re\n",
    "from matplotlib import pyplot as pp\n",
    "import nltk as nltk\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/brandon/Documents/UCL/COMP0087/CW2')\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Full Edgar Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and get unique ciks\n",
    "These come from https://www.kaggle.com/finnhub/sec-filings?select=2021.QTR1.csv which has data back to '94!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/brandon/Documents/UCL/COMP0087/CW2/archive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_files = [\n",
    "    '2020.QTR1.csv',\n",
    "    '2020.QTR2.csv',\n",
    "    '2020.QTR3.csv',\n",
    "    '2020.QTR4.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data from finhub sec filings\n",
    "ciks = []\n",
    "for fn in read_files:\n",
    "    frame = pd.read_csv('2020.QTR1.csv')\n",
    "    ciks.append(frame[frame.form=='10-K'].loc[:, ['symbol', 'cik']])\n",
    "ciks = pd.concat(ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null data\n",
    "ciks = ciks[~pd.isnull(ciks['symbol'])]\n",
    "ciks = ciks[~pd.isnull(ciks['cik'])]\n",
    "\n",
    "# multiple dupicate filings so drop these\n",
    "ciks = ciks.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have ciks for all unique 10-K filings in 2020! There are about 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciks.sort_values('symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(ciks['cik'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearly some tickers have multiple ciks\n",
    "len(np.unique(ciks['symbol'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get business description data for ciks\n",
    "We download data from EDGAR via package edgar using the ciks determined above. Then we look through the raw text to find the business description and pull it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sequence(seq, all_tokens):\n",
    "    check_num = 0\n",
    "    seq_loc = 0\n",
    "    for ll, ss in enumerate(all_tokens):\n",
    "        if ss.lower() in seq:\n",
    "            check_num += 1\n",
    "        else:\n",
    "            check_num = 0\n",
    "        # highly hacky as first location is in table of contents; this return second\n",
    "        if check_num == len(seq):\n",
    "            seq_loc = ll + 1\n",
    "    return seq_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "bds = {}\n",
    "try_starts = [\n",
    "    {'item', '1.', 'business'},\n",
    "    {'item', '1.', 'our', 'business'},\n",
    "    {'item', '1:', 'business'},\n",
    "    {'item', '1.', 'business.'},\n",
    "    {'item', '1', 'business'},\n",
    "    {'item', '1.business'},\n",
    "    \n",
    "        ]\n",
    "try_ends = [\n",
    "    {'item', '1a.', 'risk', 'factors'},\n",
    "    {'item', '1a:', 'risk', 'factors'},\n",
    "    {'item', '1a.', 'risk', 'factors.'},\n",
    "    {'item', '1a', 'risk', 'factors'},\n",
    "        ]\n",
    "max_tries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_over = [(x, y) for x, y in zip(ciks['symbol'].values, ciks['cik'].values)]\n",
    "for ticker, cik in loop_over:\n",
    "    print(f'Extracting {ticker}:{cik}')\n",
    "    tt = 0\n",
    "    while tt < max_tries:\n",
    "        try:\n",
    "            company = Company(ticker, str(cik))\n",
    "            tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "            docs = Company.get_documents(tree, no_of_documents=5)\n",
    "            break\n",
    "        except:\n",
    "            tt+=1\n",
    "    if tt == max_tries:\n",
    "        print(f'Edgar timeout for {ticker}')\n",
    "        continue\n",
    "    for doc in docs:\n",
    "        fs_text = doc.text_content()\n",
    "        # could tokenise better here\n",
    "        # re.split(r'[ \\t\\n]+', fs_text) would split on any number of tabs, whitespaces, newlines\n",
    "        fs_text = fs_text.replace('\\t', ' ')\n",
    "        fs_text = fs_text.replace('\\n', ' ')\n",
    "        tokenised = fs_text.split()\n",
    "        \n",
    "        for ts in try_starts:\n",
    "            start_seq = find_sequence(ts, tokenised)\n",
    "            if start_seq != 0:\n",
    "                break\n",
    "        for te in try_ends:\n",
    "            end_seq = find_sequence(te, tokenised)\n",
    "            if end_seq != 0:\n",
    "                break\n",
    "        checks = (start_seq != 0) & (end_seq != 0) & ((end_seq - start_seq) > 100)\n",
    "        if checks:\n",
    "            print(f'from {start_seq} to {end_seq} out of {len(tokenised)}')\n",
    "            break\n",
    "    if checks:\n",
    "        bds[f'{ticker}:{cik}'] = tokenised[start_seq:end_seq]\n",
    "    else:\n",
    "        bds[f'{ticker}:{cik}'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bds, open(\"bds_1.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/brandon/Documents/UCL/COMP0087/CW2')\n",
    "file1 = open(\"bds_1.txt\",\"a\")\n",
    "for kk, vv in bds.items():\n",
    "    file1.write(kk + '\\n')\n",
    "    file1.write(' '.join(vv)+ '\\n')\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for vv in bds.values():\n",
    "    if len(vv) > 0:\n",
    "        i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(cik, doc_num):\n",
    "    company = Company(str(cik), str(cik))\n",
    "    tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "    docs = Company.get_documents(tree, no_of_documents=doc_num+1)\n",
    "    fs_text = doc.text_content()\n",
    "    fs_text = fs_text.replace('\\t', ' ')\n",
    "    fs_text = fs_text.replace('\\n', ' ')\n",
    "    tokenised = fs_text.split()\n",
    "    print(' '.join(tokenised)[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(1000753, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly some extractions have failed, about 40% currently which is too high. These will require a case by case handling. Below is a problem for novavax. Also sometimes edgar failes as per error message abover (we could just try catch this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding start sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(cik, doc_num):\n",
    "    company = Company(str(cik), str(cik))\n",
    "    tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "    docs = Company.get_documents(tree, no_of_documents=doc_num+1)\n",
    "    fs_text = doc.text_content()\n",
    "    fs_text = fs_text.replace('\\t', ' ')\n",
    "    fs_text = fs_text.replace('\\n', ' ')\n",
    "    return fs_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = get_tokens(1000697, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smoothed(tokens, string, smoothing):\n",
    "    contains = np.array([x.lower().find(string) != -1 for x in tokens])\n",
    "    return np.minimum(np.convolve(contains, np.ones(smoothing)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 3\n",
    "loc_business = find_smoothed(all_tokens, 'business', win)\n",
    "loc_item = find_smoothed(all_tokens, 'item', win)\n",
    "loc_1 = find_smoothed(all_tokens, '1', win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(loc_business+loc_item+loc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win=3\n",
    "loc_item = find_smoothed(all_tokens, 'item', win)\n",
    "loc_1a = find_smoothed(all_tokens, '1a', win)\n",
    "loc_risk = find_smoothed(all_tokens, 'risk', win)\n",
    "loc_factors = find_smoothed(all_tokens, 'factors', win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(loc_item+ loc_1a+loc_risk+loc_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgar Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Classify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/ClassifiedSections'\n",
    "os.chdir(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tickers = set([x.split('_')[0] for x in os.listdir(my_path)])\n",
    "\n",
    "loop_over = [(x, y) for x, y in zip(ciks['symbol'].values, ciks['cik'].values)]\n",
    "for ticker, cik in loop_over:\n",
    "    if ticker in tickers:\n",
    "        continue\n",
    "    company = Company(ticker, str(cik))\n",
    "    tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "    docs = Company.get_documents(tree, no_of_documents=5)\n",
    "    fs_text = docs[0].text_content().lower()\n",
    "    \n",
    "    item_symbol = re.compile('item\\s\\d.')\n",
    "    sections = item_symbol.split(fs_text)\n",
    "    \n",
    "    found_bd = False\n",
    "    \n",
    "    for i, section in enumerate(sections):\n",
    "        tokenised = re.split(r'[\\s\\n]+', section)\n",
    "        if (len(tokenised) > 2000) and (len(tokenised) < 200000):\n",
    "            if not found_bd:\n",
    "                decision = input(' '.join(tokenised[:50]))\n",
    "            if decision == '0' or found_bd:\n",
    "                # not a business description\n",
    "                fn = f'{ticker}_section{i}_0.txt'\n",
    "            else:\n",
    "                fn = f'{ticker}_section{i}_1.txt'\n",
    "                found_bd = True\n",
    "            f = open(fn, 'a')\n",
    "            f.write(section)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([x.split('_')[0] for x in os.listdir(my_path)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_subspace = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(my_path)\n",
    "y = []\n",
    "X_raw = []\n",
    "for fn in file_names:\n",
    "    fn_first = fn.split('.')[0]\n",
    "    y.append(int(fn_first.split('_')[2]))\n",
    "    f = open(fn, 'r')\n",
    "    f_text = f.read()\n",
    "    f.close()\n",
    "    X_raw.append(f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_raw, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf_vectorizer.fit(X_train)\n",
    "\n",
    "X_train = tf_vectorizer.transform(X_train)\n",
    "X_valid = tf_vectorizer.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=n_subspace, n_iter=7, random_state=42)\n",
    "svd.fit(X_train)\n",
    "\n",
    "X_train = svd.transform(X_train)\n",
    "X_valid = svd.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_classifier = SVC(C=1, kernel='rbf', class_weight='balanced')\n",
    "svc_classifier = SVC(C=1, kernel='rbf', gamma=10, class_weight={0: 1, 1: 2.5})\n",
    "svc_classifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc_classifier, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc_classifier, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem here is getting classifer to be sparse, but not too sparse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier.predict(X_train)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Scrape and Classify\n",
    "\n",
    "## Note could search all documents!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/AutoClassifiedSections'\n",
    "os.chdir(my_path)\n",
    "\n",
    "loop_over = [(x, y) for x, y in zip(ciks['symbol'].values, ciks['cik'].values)]\n",
    "\n",
    "while 1:\n",
    "    passed = True\n",
    "#     try:\n",
    "    tickers = set([x.split('_')[0] for x in os.listdir(my_path)])\n",
    "    for ticker, cik in loop_over:\n",
    "        print(f'Processing {ticker}')\n",
    "        if ticker in tickers:\n",
    "            continue\n",
    "        company = Company(ticker, str(cik))\n",
    "        tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "        docs = Company.get_documents(tree, no_of_documents=5)\n",
    "        fs_text = docs[0].text_content().lower()\n",
    "\n",
    "        item_symbol = re.compile('item\\s\\d.')\n",
    "        sections = item_symbol.split(fs_text)\n",
    "        lengths = [len(re.split(r'[\\s\\n]+', x)) for x in sections]\n",
    "        sections = [x for x, y in zip(sections, lengths) if (y > 2000) and (y < 200000)]\n",
    "        print(f'Found {len(sections)} sections')\n",
    "\n",
    "        X_classify = tf_vectorizer.transform(sections)\n",
    "        X_classify = svd.transform(X_classify)\n",
    "        X_classify = scaler.transform(X_classify)\n",
    "        y_hat = svc_classifier.predict(X_classify)\n",
    "\n",
    "        for i, section in enumerate(sections):\n",
    "            if y_hat[i] == 0:\n",
    "                # not a business description\n",
    "                fn = f'{ticker}_section{i}_0.txt'\n",
    "            else:\n",
    "                fn = f'{ticker}_section{i}_1.txt'\n",
    "            f = open(fn, 'a')\n",
    "            f.write(section)\n",
    "            f.close()\n",
    "                \n",
    "#     except:\n",
    "#         passed = False\n",
    "        \n",
    "    if passed:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier.predict(X_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â LDA Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/brandon/Documents/UCL/COMP0087/CW2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"bds_1.txt\", \"r\")\n",
    "f_lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_ids = f_lines[0::2]\n",
    "company_descriptions = f_lines[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_descriptions = [len(x) for x in company_descriptions]\n",
    "plt.figure(figsize=[10, 5])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(len_descriptions, bins=100);\n",
    "plt.title('Description Lengths');\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist([len(x) for x in company_ids], bins=100);\n",
    "plt.title('Id Lengths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_descriptions = np.array(len_descriptions) > 3000\n",
    "n_samples = np.sum(valid_descriptions)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_ids = np.array(company_ids)[valid_descriptions]\n",
    "company_descriptions = [x for x in company_descriptions if len(x) > 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_lengths = [len(x) for x in company_descriptions if len(x) < 5e5]\n",
    "plt.hist(some_lengths, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction via Term Frequency\n",
    "This handles special characters, takes lowercase and removes common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(company_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting LDA\n",
    "Online LDA using VB. Really fast!\n",
    "If there are W words in dictionary, D documents in corpus and K topics then it learns:\n",
    " - a matrix K $\\times$ W of word probabilities per topic that allow you to determine what the topics mean\n",
    " - a matrix D $\\times$ K of topic probabilities per document that allows you to cluster documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.perplexity(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the Topics Mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# names of words \n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigining Topics to Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a companies by topics matrix of prob of each topic by company\n",
    "topics_companies = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_topics = np.argmax(topics_companies, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking out companies in topic 1 i.e. pharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 1\n",
    "topic_locs = np.argwhere(primary_topics==topic_id)\n",
    "\n",
    "this_ind = 5\n",
    "print(company_ids[topic_locs[this_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(company_descriptions[int(topic_locs[this_ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now topic 9 oil & gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 9\n",
    "topic_locs = np.argwhere(primary_topics==topic_id)\n",
    "\n",
    "this_ind = 5\n",
    "print(company_ids[topic_locs[this_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(company_descriptions[int(topic_locs[this_ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now topic 6 ... REITS. Note how high up COVID is in this description and consider its impact on companies that lease office or retail space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 6\n",
    "topic_locs = np.argwhere(primary_topics==topic_id)\n",
    "\n",
    "this_ind = 5\n",
    "print(company_ids[topic_locs[this_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(company_descriptions[int(topic_locs[this_ind])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_ind = 10\n",
    "# prison REIT lol\n",
    "print(company_descriptions[int(topic_locs[this_ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, vec = np.linalg.eig(topics_companies.T @ topics_companies)\n",
    "projection_01 = topics_companies @ vec[:, 0:2]\n",
    "projection_12 = topics_companies @ vec[:, 1:3]\n",
    "projection_23 = topics_companies @ vec[:, 2:4]\n",
    "projection_34 = topics_companies @ vec[:, 3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 15])\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(projection_01[primary_topics==1, :][:, 0], projection_01[primary_topics==1, :][:, 1])\n",
    "plt.scatter(projection_01[primary_topics==4, :][:, 0], projection_01[primary_topics==4, :][:, 1])\n",
    "plt.scatter(projection_01[primary_topics==6, :][:, 0], projection_01[primary_topics==6, :][:, 1])\n",
    "plt.scatter(projection_01[primary_topics==9, :][:, 0], projection_01[primary_topics==9, :][:, 1])\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(projection_12[primary_topics==1, :][:, 0], projection_12[primary_topics==1, :][:, 1])\n",
    "plt.scatter(projection_12[primary_topics==4, :][:, 0], projection_12[primary_topics==4, :][:, 1])\n",
    "plt.scatter(projection_12[primary_topics==6, :][:, 0], projection_12[primary_topics==6, :][:, 1])\n",
    "plt.scatter(projection_12[primary_topics==9, :][:, 0], projection_12[primary_topics==9, :][:, 1])\n",
    "plt.legend(['Pharma', 'Banks', 'REITs', 'Energy'])\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(projection_23[primary_topics==1, :][:, 0], projection_23[primary_topics==1, :][:, 1])\n",
    "plt.scatter(projection_23[primary_topics==4, :][:, 0], projection_23[primary_topics==4, :][:, 1])\n",
    "plt.scatter(projection_23[primary_topics==6, :][:, 0], projection_23[primary_topics==6, :][:, 1])\n",
    "plt.scatter(projection_23[primary_topics==9, :][:, 0], projection_23[primary_topics==9, :][:, 1])\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(projection_34[primary_topics==1, :][:, 0], projection_34[primary_topics==1, :][:, 1])\n",
    "plt.scatter(projection_34[primary_topics==4, :][:, 0], projection_34[primary_topics==4, :][:, 1])\n",
    "plt.scatter(projection_34[primary_topics==6, :][:, 0], projection_34[primary_topics==6, :][:, 1])\n",
    "plt.scatter(projection_34[primary_topics==9, :][:, 0], projection_34[primary_topics==9, :][:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_select = np.zeros(len(topics_companies), dtype=bool)\n",
    "sub_select[:50] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = topics_companies[sub_select, :] @ topics_companies[sub_select, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(adj_matrix, 1)\n",
    "am_cut = 0.2\n",
    "adj_matrix[adj_matrix < am_cut] = 0\n",
    "adj_matrix[adj_matrix >= am_cut] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(adj_matrix)\n",
    "these_names = company_ids[sub_select]\n",
    "node_mapping = {i: x for i, x in enumerate(these_names)}\n",
    "G = nx.relabel_nodes(G, node_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 15])\n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Selecting K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"bds_1.txt\", \"r\")\n",
    "f_lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "company_ids = f_lines[0::2]\n",
    "company_descriptions = f_lines[1::2]\n",
    "\n",
    "len_descriptions = [len(x) for x in company_descriptions]\n",
    "valid_descriptions = np.array(len_descriptions) > 3000\n",
    "company_ids = np.array(company_ids)[valid_descriptions]\n",
    "\n",
    "company_descriptions = [x for x in company_descriptions if len(x) > 3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(company_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "lda_10 = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_10.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 15\n",
    "lda_15 = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_15.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20\n",
    "lda_20 = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_20.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Measure\n",
    "See http://qpleple.com/topic-coherence-to-evaluate-topic-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dwi = np.array(np.sum(tf > 0, axis=0))[0]\n",
    "\n",
    "W_bin = tf\n",
    "W_bin[W_bin > 0] = 1\n",
    "Dwi_wj = W_bin.T @ W_bin\n",
    "\n",
    "score_umass = np.log((Dwi_wj.toarray() + 1)/ Dwi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_coherence(model, n_top_words, pair_score):\n",
    "    coherences = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        coh = 0\n",
    "        for i in range(len(top_features_ind)):\n",
    "            for j in range(i):\n",
    "                coh += pair_score[top_features_ind[i], top_features_ind[j]]\n",
    "        coherences.append(coh)\n",
    "    return coherences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Coherence for Different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "tc_values = []\n",
    "runs = 20\n",
    "for k in k_values:\n",
    "    print(f'running k = {k}')\n",
    "    this_c = []\n",
    "    for run in range(runs):\n",
    "    \n",
    "        lda = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "        lda.fit(tf)\n",
    "        this_c.append(np.median(topic_coherence(lda, 10, score_umass)))\n",
    "    tc_values.append(this_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, tc_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    K = len(model.components_)\n",
    "    n_x = 5\n",
    "    n_y = int(np.ceil(K / n_x))\n",
    "    fig, axes = plt.subplots(n_y, n_x, figsize=(3 * n_x, 7.5 * n_y), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=30)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "plot_top_words(lda_10, tf_feature_names, 10, 'Topics in LDA 10 model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(lda_15, tf_feature_names, 10, 'Topics in LDA 15 model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_top_words(lda_20, tf_feature_names, 10, 'Topics in LDA 20 model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pp\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Description Data\n",
    "### SP500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = Path('/Users/brandon/Documents/UCL/COMP0087/CW2/SP500')\n",
    "os.chdir(my_path)\n",
    "fns = os.listdir(my_path)\n",
    "\n",
    "ticker_sp50 = []\n",
    "sector_sp50 = []\n",
    "bds_sp50 = []\n",
    "for fn in fns:\n",
    "    fn_first = fn.split('.txt')[0]\n",
    "    ticker = fn_first.split('_')[0]\n",
    "    sector = fn_first.split('_')[1]\n",
    "    f = open(fn, 'r', encoding=\"utf8\")\n",
    "    f_text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    ticker_sp50.append(ticker)\n",
    "    sector_sp50.append(sector)\n",
    "    bds_sp50.append(f_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Dataset - excluding SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path('/Users/brandon/Documents/UCL/COMP0087/CW2'))\n",
    "f = open(\"bds_1.txt\", \"r\", encoding=\"utf8\")\n",
    "f_lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "company_ids_all = f_lines[0::2]\n",
    "company_descriptions_all = f_lines[1::2]\n",
    "company_tickers = [x.split(':')[0] for x in company_ids_all]\n",
    "removeSP = np.in1d(np.array(company_tickers), list(ticker_sp50))\n",
    "\n",
    "bds_all = []\n",
    "ticker_all = []\n",
    "for i, d in enumerate(company_descriptions_all):\n",
    "    if (len(d) > 3000) and not removeSP[i]:\n",
    "        bds_all.append(d)\n",
    "        ticker_all.append(company_ids_all[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Returns Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_d = np.datetime64('2018-01-01')\n",
    "end_d = np.datetime64('2020-01-01')\n",
    "business_ds = pd.date_range(start_d, end_d, freq='B')\n",
    "\n",
    "my_path = Path('/Users/brandon/Documents/UCL/COMP0087/CW2')\n",
    "os.chdir(my_path)\n",
    "price_data = pd.read_csv('Price.csv')\n",
    "\n",
    "select_these = np.in1d(price_data.tic.values, list(ticker_sp50))\n",
    "price_sp50 = price_data.loc[select_these, ['tic', 'datadate', 'prccd']]\n",
    "price_sp50['datadate'] = pd.to_datetime(price_sp50['datadate'], format='%Y%m%d')\n",
    "price_sp50 = pd.pivot_table(price_sp50,index='datadate',columns='tic',values='prccd')\n",
    "price_sp50 = price_sp50.ffill(limit=5)\n",
    "price_sp50 = price_sp50.reindex(business_ds)\n",
    "price_sp50 = price_sp50.dropna(axis=0)\n",
    "\n",
    "returns_sp50 = np.log(price_sp50) - np.log(price_sp50.shift(1))\n",
    "returns_sp50 = returns_sp50.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Returns with LDA\n",
    "### Train LDA on Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(bds_all)\n",
    "\n",
    "n_components = 20\n",
    "lda_20 = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_20.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features for S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = tf_vectorizer.transform(bds_sp50)\n",
    "\n",
    "features_sp50 = lda_20.transform(tf_sp50)\n",
    "\n",
    "features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Over Dates and Perform OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = returns_sp50.index\n",
    "max_f = 19\n",
    "adj_r2_features = []\n",
    "\n",
    "for dd in all_dates:\n",
    "    reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "    y = reg_data[:, 0]\n",
    "    X = reg_data[:, 1:]\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    X = std_scaler.fit_transform(X)\n",
    "\n",
    "    X = sm.add_constant(X, prepend=False)\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    res = ols_model.fit()\n",
    "    adj_r2_features.append(res.rsquared_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(all_dates, adj_r2_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate average regression adjusted $R^2$ for different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = tf_vectorizer.transform(bds_sp50)\n",
    "all_ks = [5, 10, 15, 20, 25, 30, 40, 50]\n",
    "num_trials = 20\n",
    "all_adj_r2 = []\n",
    "for k in all_ks:\n",
    "    print(f'Running for k = {k}')\n",
    "    adj_r2_k = []\n",
    "    for t in range(num_trials):\n",
    "        this_lda = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.)\n",
    "        this_lda.fit(tf)\n",
    "    \n",
    "        features_sp50 = this_lda.transform(tf_sp50)\n",
    "        features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)\n",
    "        adj_r2_dates = []\n",
    "        for dd in all_dates:\n",
    "            reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "            y = reg_data[:, 0]\n",
    "            X = reg_data[:, 1:]\n",
    "\n",
    "            std_scaler = StandardScaler()\n",
    "            X = std_scaler.fit_transform(X)\n",
    "\n",
    "            X = sm.add_constant(X, prepend=False)\n",
    "            ols_model = sm.OLS(y, X)\n",
    "            res = ols_model.fit()\n",
    "            adj_r2_dates.append(res.rsquared_adj)\n",
    "        adj_r2_k.append(np.mean(adj_r2_dates))\n",
    "    all_adj_r2.append(adj_r2_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(x) for x in all_adj_r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.std(x) for x in all_adj_r2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pp\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import string\n",
    "\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans on S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance of Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess larger dataset\n",
    "t0 = time()\n",
    "#Remove puncuation\n",
    "bds_all2 = bds_all[:]\n",
    "for i in range(len(bds_all)):\n",
    "    for k in string.punctuation:\n",
    "        bds_all2[i] = bds_all2[i].replace(k, '')\n",
    "\n",
    "#Tokenise and remove stopwords\n",
    "tokenised = []\n",
    "for i in range(len(bds_all2)):\n",
    "    text = bds_all2[i].split()\n",
    "    text = [word.lower() for word in text if not word.lower() in stopwords.words('english')]\n",
    "    tokenised.append(text)\n",
    "\n",
    "#Lemmatizes each word\n",
    "lemmad_text = []\n",
    "for i in range(len(tokenised)): #CHANGE THIS\n",
    "# for i in range(1):\n",
    "    print(i)\n",
    "    lemmad_array = tokenised[i]\n",
    "    lemmad = [lemmatizer.lemmatize(word.lower()) for word in lemmad_array]\n",
    "    lemmad = ' '.join(lemmad)\n",
    "    lemmad_text.append(lemmad)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(lemmad_text[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess sp500\n",
    "t0 = time()\n",
    "#Remove puncuation\n",
    "bds_sp50_2 = bds_sp50[:]\n",
    "for i in range(len(bds_sp50_2)):\n",
    "    for k in string.punctuation:\n",
    "        bds_sp50_2[i] = bds_sp50_2[i].replace(k, '')\n",
    "\n",
    "#Tokenise and remove stopwords\n",
    "tokenised_sp50 = []\n",
    "for i in range(len(bds_sp50_2)):\n",
    "    text = bds_sp50_2[i].split()\n",
    "    text = [word.lower() for word in text if not word.lower() in stopwords.words('english')]\n",
    "    tokenised_sp50.append(text)\n",
    "\n",
    "#Lemmatizes each word\n",
    "lemmad_text_sp50 = []\n",
    "for i in range(len(tokenised_sp50)): #CHANGE THIS\n",
    "# for i in range(1):\n",
    "    print(i)\n",
    "    lemmad_array = tokenised_sp50[i]\n",
    "    lemmad = [lemmatizer.lemmatize(word.lower()) for word in lemmad_array]\n",
    "    lemmad = ' '.join(lemmad)\n",
    "    lemmad_text_sp50.append(lemmad)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(lemmad_text_sp50[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lem = CountVectorizer()\n",
    "X_lemmad = cv_lem.fit_transform(lemmad_text)\n",
    "print(X_lemmad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Kmeans using elbow method to find best k based on inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "#Training Kmeans\n",
    "wcss_lemmad = [] #CHANGE\n",
    "for i in range(1, 20): #max of 20 clusters otherwise might take too long\n",
    "    kmeans_lem = KMeans(n_clusters = i, init='k-means++', max_iter = 300, n_init = 10, random_state = 0, verbose = True)\n",
    "    kmeans_lem.fit(X_lemmad) #CHANGE\n",
    "    wcss_lemmad.append(kmeans_lem.inertia_) #CHANGE\n",
    "    \n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "    \n",
    "#Visualisation of inertia (how far away the points within a cluster are, want small inertia)\n",
    "pp.plot(range(1,20),wcss_lemmad[0:20]) #CHANGE\n",
    "pp.title('the elbow method')\n",
    "pp.xlabel('no of clusters')\n",
    "pp.ylabel('wcss')\n",
    "pp.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using best k from elbow method on larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying K-means based on above elbow method, can change true_k to the desired number of clusters\n",
    "true_k = 10\n",
    "model_true = KMeans(n_clusters=true_k, init='k-means++', n_init=1)\n",
    "model_true.fit(X_lemmad) #CHANGE\n",
    "\n",
    "#Print results\n",
    "print(\"top terms for cluster:\")\n",
    "order_centroids_lem = model_true.cluster_centers_.argsort()[:, ::-1]\n",
    "terms_lem = cv_lem.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"cluster %d:\" % i),\n",
    "    for ind in order_centroids_lem[i, :10]: #selects top terms\n",
    "        print(' %s' % terms_lem[ind]),\n",
    "    print\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Returns with Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features for S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = cv_lem.transform(lemmad_text_sp50)\n",
    "\n",
    "features_sp50 = model_true.transform(tf_sp50)\n",
    "\n",
    "features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Over Dates and Perform OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = returns_sp50.index\n",
    "max_f = 19\n",
    "adj_r2_features = []\n",
    "\n",
    "for dd in all_dates:\n",
    "    reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "    y = reg_data[:, 0]\n",
    "    X = reg_data[:, 1:]\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    X = std_scaler.fit_transform(X)\n",
    "\n",
    "    X = sm.add_constant(X, prepend=False)\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    res = ols_model.fit()\n",
    "    adj_r2_features.append(res.rsquared_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(all_dates, adj_r2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = cv_lem.transform(lemmad_text_sp50)\n",
    "all_ks = [5, 10, 15, 20, 25, 30, 40, 50]\n",
    "num_trials = 20\n",
    "all_adj_r2 = []\n",
    "for k in all_ks:\n",
    "    print(f'Running for k = {k}')\n",
    "    adj_r2_k = []\n",
    "    for t in range(num_trials):\n",
    "        \n",
    "        this_kmeans = KMeans(n_clusters = k, init='k-means++', n_init = 1, random_state = 0, verbose = True)\n",
    "        this_kmeans.fit(X_lemmad)\n",
    "        \n",
    "        features_sp50 = this_kmeans.transform(tf_sp50)\n",
    "        features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)\n",
    "        \n",
    "#         this_lda = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "#                                     learning_method='online',\n",
    "#                                     learning_offset=50.)\n",
    "#         this_lda.fit(tf)\n",
    "    \n",
    "#         features_sp50 = this_lda.transform(tf_sp50)\n",
    "#         features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)\n",
    "        adj_r2_dates = []\n",
    "        for dd in all_dates:\n",
    "            reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "            y = reg_data[:, 0]\n",
    "            X = reg_data[:, 1:]\n",
    "\n",
    "            std_scaler = StandardScaler()\n",
    "            X = std_scaler.fit_transform(X)\n",
    "\n",
    "            X = sm.add_constant(X, prepend=False)\n",
    "            ols_model = sm.OLS(y, X)\n",
    "            res = ols_model.fit()\n",
    "            adj_r2_dates.append(res.rsquared_adj)\n",
    "        adj_r2_k.append(np.mean(adj_r2_dates))\n",
    "    all_adj_r2.append(adj_r2_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(x) for x in all_adj_r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[np.std(x) for x in all_adj_r2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVDM Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Command line version of notebook.\n",
    "\"\"\"\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "import data_handling as data\n",
    "from models import nvdm\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-k\", type=int, dest=\"num_topics\", required=True)\n",
    "parser.add_argument(\"-e\", type=int, dest=\"num_epochs\", required=True)\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "print(\"Load data..\")\n",
    "\n",
    "# Data that has already been preprocessed\n",
    "# Generated by applying pp.preprocess_text() to each BD,\n",
    "# then saved to a TSV\n",
    "DATA_CLEAN_PATH = \"./data/bds_1_clean.txt\"\n",
    "IDs_raw, BDs_raw = data.load_raw(DATA_CLEAN_PATH)\n",
    "\n",
    "# Some entries have empty BDs, so filter those out\n",
    "IDs = []\n",
    "BDs = []\n",
    "for iid, bd in zip(IDs_raw, BDs_raw):\n",
    "    if len(bd) > 0:\n",
    "        IDs.append(iid)\n",
    "        BDs.append(bd)\n",
    "\n",
    "print(len(IDs), len(BDs))\n",
    "\n",
    "# Build frequency table\n",
    "# (cleaned data joins tokens by space)\n",
    "counter = Counter()\n",
    "for desc in BDs:\n",
    "    counter.update(desc.split(\" \"))\n",
    "\n",
    "# PyTorch torchtext vocabulary converts tokens to indices and vice versa.\n",
    "# Also has an '<unk>' for OOV words (might be useful later).\n",
    "# vocab = Vocab(counter,\n",
    "#               max_size=10000,\n",
    "#               min_freq=1,\n",
    "#               specials=['<unk>'])\n",
    "# print(len(vocab))\n",
    "# Load the same precomputed vocabulary each time for consistency.\n",
    "with open(\"./vocabs/vocab_bds_1_clean_10000.pickle\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "\n",
    "class BDDataset(Dataset):\n",
    "    \"\"\" Very simple dataset object. Stores all the passages.\n",
    "    \n",
    "    This is just for compatibility with PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# \"Preprocessing\" function: just splits the text\n",
    "# The file's text is already preprocessed.\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in text.split(\" \")]\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Convert a batch of text (each a list of tokens) into appropriate torch tensors.\n",
    "    \n",
    "    Modification of https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html.\n",
    "    We don't need labels.\n",
    "    \"\"\"\n",
    "    # Offsets tells the model (which will use EmbeddingBag) where each text starts.\n",
    "    text_list, offsets = [], [0]\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), offsets.to(device)\n",
    "\n",
    "# Create data loader to iterate over dataset in batches during training/evaluation\n",
    "dataset = BDDataset(BDs)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=collate_batch)\n",
    "hidden_size = 500\n",
    "num_topics = args.num_topics\n",
    "\n",
    "# Training setup\n",
    "\n",
    "# Total number of epochs\n",
    "outer_epochs = args.num_epochs\n",
    "\n",
    "model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Trains both the encoder and decoder at the same time.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "print(\"Start training...\")\n",
    "\n",
    "\n",
    "for epoch in range(outer_epochs):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    rec_sum = 0.0\n",
    "    kl_sum = 0.0\n",
    "    n = len(data_loader)\n",
    "\n",
    "    for idx, (text, offsets) in enumerate(data_loader):\n",
    "        text = text.to(device)\n",
    "        offsets = offsets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(text, offsets, kl_weight=1.0)\n",
    "        loss = loss_dict[\"total\"].sum()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # For printing\n",
    "        loss_sum += loss.item()\n",
    "        rec_sum += loss_dict[\"rec\"].sum().item()\n",
    "        kl_sum += loss_dict[\"kl\"].sum().item()\n",
    "\n",
    "    print(f\"[Time: {timedelta(seconds=timer() - start_time)}, Epoch {epoch + 1}] Loss {loss_sum/n}, Rec {rec_sum/n}, KL {kl_sum/n}\")\n",
    "\n",
    "MODELSAVE_PATH = f\"./modelsaves/nvdm_k{num_topics}_{outer_epochs}epochs.pt\"\n",
    "print(f\"Save model... to {MODELSAVE_PATH}\")\n",
    "torch.save(model.state_dict(), MODELSAVE_PATH)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "PORTER_STEMMER = PorterStemmer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Preprocess one line of text. \"\"\"\n",
    "    # NLTK also separates out punctuation, but keeps them in the list.\n",
    "    # e.g. ',' is a token.\n",
    "    tokens = nltk.word_tokenize(text, language=\"english\")\n",
    "\n",
    "    tokens_new = []\n",
    "    for token in tokens:\n",
    "        if token in STOPWORDS:\n",
    "            continue\n",
    "\n",
    "        if not token.isalnum():\n",
    "            # Only keep words containing alphanumeric characters\n",
    "            # Tokens containing only punctuation, symbols etc. are removed\n",
    "            continue\n",
    "\n",
    "        # TODO: do further preprocessing if necessary,\n",
    "        #       (if vocab size is too large) e.g. handle Unicode, lowercase\n",
    "\n",
    "        # Stem word\n",
    "        # TODO: map stemmed words to their original to recover them???\n",
    "        token_new = PORTER_STEMMER.stem(token)\n",
    "        tokens_new.append(token_new)\n",
    "\n",
    "    return tokens_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load text files.\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "\n",
    "def load_raw(path):\n",
    "    \"\"\" Load data in the same format as bds_1.txt. \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.read().split(\"\\n\")\n",
    "\n",
    "    ids = []\n",
    "    bds = []\n",
    "    for i, p in enumerate(lines):\n",
    "        if i % 2 == 0:\n",
    "            ids.append(p)\n",
    "        else:\n",
    "            bds.append(p)\n",
    "\n",
    "    # Original bds_1.txt has a newline at the end of file,\n",
    "    # so remove this blank line\n",
    "    if len(lines) % 2 != 0 and ids[-1] == '':\n",
    "        ids.pop(-1)\n",
    "\n",
    "    return ids, bds\n",
    "\n",
    "\n",
    "def write_data(ids, bds, path):\n",
    "    \"\"\" Write some ID/BD pairs to a tab-separate file. \"\"\"\n",
    "    # Python docs says newline=\"\" must be specified for csv\n",
    "    assert len(ids) == len(bds)\n",
    "\n",
    "    with open(path, \"w\", newline=\"\") as outfile:\n",
    "        for iid, bd in zip(ids, bds):\n",
    "            outfile.write(iid + \"\\n\")\n",
    "            outfile.write(\" \".join(bd) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVDM Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch stuff\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autoselect target device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Custom modules.\n",
    "\n",
    "import data_handling as data\n",
    "import preprocess as pp\n",
    "from models import nvdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original data\n",
    "# DATA_RAW_PATH = \"./data/bds_1.txt\"\n",
    "# IDs, BDs = data.load_raw(DATA_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that has already been preprocessed, same format as original bds_1.txt.\n",
    "# Generated by applying pp.preprocess_text() to each BD and saving this to a text file.\n",
    "DATA_CLEAN_PATH = \"./data/bds_1_clean.txt\"\n",
    "IDs_raw, BDs_raw = data.load_raw(DATA_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some entries have empty BDs, so filter those out\n",
    "IDs = []\n",
    "BDs = []\n",
    "for iid, bd in zip(IDs_raw, BDs_raw):\n",
    "    if len(bd) > 0:\n",
    "        IDs.append(iid)\n",
    "        BDs.append(bd)\n",
    "\n",
    "print(len(IDs), len(BDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows how the torchtext Vocab was created.\n",
    "# The object is saved to disk, so this cell only needs to uncommented and run once.\n",
    "# Build frequency table\n",
    "# (cleaned data joins tokens by space)\n",
    "# counter = Counter()\n",
    "# for desc in BDs:\n",
    "#     counter.update(desc.split(\" \"))\n",
    "\n",
    "# PyTorch torchtext vocabulary converts tokens to indices and vice versa.\n",
    "# Also has an '<unk>' for OOV words (might be useful later).\n",
    "# vocab = Vocab(counter,\n",
    "#               max_size=10000,\n",
    "#               min_freq=1,\n",
    "#               specials=['<unk>'])\n",
    "# print(len(vocab))\n",
    "# actual is 70770 without max_size restriction\n",
    "\n",
    "# Save the vocab to file\n",
    "# with open(\"./vocabs/vocab_bds_1_clean_10000.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the vocab was already created, load the vocab from file.\n",
    "with open(\"./vocabs/vocab_bds_1_clean_10000.pickle\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDataset(Dataset):\n",
    "    \"\"\" Very simple dataset object. Stores all the passages.\n",
    "    \n",
    "    This is just for compatibility with PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Preprocessing\" function: just splits the text\n",
    "# The file's text is already preprocessed.\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in text.split(\" \")]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Convert a batch of text (each a list of tokens) into appropriate torch tensors.\n",
    "    \n",
    "    Modification of https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html.\n",
    "    We don't need labels.\n",
    "    \"\"\"\n",
    "    # Offsets tells the model (which will use EmbeddingBag) where each text starts.\n",
    "    text_list, offsets = [], [0]\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader to iterate over dataset in batches during training/evaluation\n",
    "dataset = BDDataset(BDs)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "hidden_size = 500\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "# Better to run run_nvdm.py in the command line to train than here.\n",
    "# Uncomment the code below to train if really needed.\n",
    "\n",
    "# # Total number of epochs\n",
    "# outer_epochs = 200\n",
    "\n",
    "# # Epochs for training the encoder/decoder on each alternation.\n",
    "# inner_epochs = 1\n",
    "\n",
    "# model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, device)\n",
    "# model = model.to(device)\n",
    "# model.train()\n",
    "\n",
    "# # Trains both the encoder and decoder at the same time.\n",
    "# # The original paper alternates between the encoder and decoder,\n",
    "# # but this is very unstable and hard to converge, from observation.\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# start_time = timer()\n",
    "\n",
    "# for epoch in range(outer_epochs):\n",
    "\n",
    "#     loss_sum = 0.0\n",
    "#     rec_sum = 0.0\n",
    "#     kl_sum = 0.0\n",
    "#     n = len(data_loader)\n",
    "\n",
    "#     for idx, (text, offsets) in enumerate(data_loader):\n",
    "#         text = text.to(device)\n",
    "#         offsets = offsets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_dict = model(text, offsets, kl_weight=1.0)\n",
    "#         loss = loss_dict[\"total\"].sum()\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # For printing\n",
    "#         loss_sum += loss.item()\n",
    "#         rec_sum += loss_dict[\"rec\"].sum().item()\n",
    "#         kl_sum += loss_dict[\"kl\"].sum().item()\n",
    "\n",
    "#     model_str = \"All\" # \"Enc\" if switch == 0 else \"Dec\"\n",
    "#     print(f\"[Time: {timedelta(seconds=timer() - start_time)}, Epoch {epoch + 1}] Loss {loss_sum/n}, Rec {rec_sum/n}, KL {kl_sum/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/save model.\n",
    "\n",
    "# MODELSAVE_PATH = \"./modelsaves/nvdm_from_notebook.pt\"\n",
    "MODELSAVE_PATH = \"./modelsaves/nvdm_k10_300epochs.pt\"\n",
    "\n",
    "# Save the trained model to file if needed.\n",
    "# torch.save(model.state_dict(), MODELSAVE_PATH)\n",
    "\n",
    "# Otherwise load the model from file if it exists.\n",
    "# Customize the n_topic (number of topics) according to the file name.\n",
    "# e.g. \"nvdm_k10_300epochs.pt\" means n_topic=k=10.\n",
    "model = nvdm.NVDM(len(vocab), 500, 10, 1, device)\n",
    "model.load_state_dict(torch.load(MODELSAVE_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocab-topic matrix (known as R in the paper).\n",
    "# It has dimensions |V| x K: vocab size x number of topics\n",
    "decoder = model.decoder[0]\n",
    "weights = decoder.weight.data.detach().clone()\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some words\n",
    "# manual KNN\n",
    "from nltk.stem import PorterStemmer\n",
    "PORTER_STEMMER = PorterStemmer()\n",
    "\n",
    "# Set of words used in the original paper\n",
    "candidates = [\"weapons\", \"medical\", \"companies\", \"define\", \"israel\", \"book\"]\n",
    "\n",
    "for candidate in candidates:\n",
    "    test_word = PORTER_STEMMER.stem(candidate)\n",
    "    idx = vocab.stoi[test_word]\n",
    "    print(test_word, idx)\n",
    "\n",
    "    # Show top 10 most similar (based on cosine distance)\n",
    "    sims = F.cosine_similarity(weights[idx].unsqueeze(0), weights)\n",
    "    sim_vals, sim_idxs = torch.topk(sims, 15)\n",
    "\n",
    "    # Show ith nearest word and its score.\n",
    "    for i, v in zip(sim_idxs, sim_vals):\n",
    "        print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "    \n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at most similar words per topic vector.\n",
    "\n",
    "V, K = weights.size()\n",
    "for i in range(K):\n",
    "    print(f\"Topic {i+1}\")\n",
    "    vals, idxs = torch.topk(torch.abs(weights[:, i]), 30)\n",
    "    for i, v in zip(idxs, vals):\n",
    "        print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(model):\n",
    "    \"\"\" Qualitative analysis of topic model. \"\"\"\n",
    "    \n",
    "    PORTER_STEMMER = PorterStemmer()\n",
    "    # Set of words used in the original NVDM paper\n",
    "    candidates = [\"weapons\", \"medical\", \"companies\", \"define\", \"israel\", \"book\"]\n",
    "    \n",
    "    # Extract the vocab-topic matrix (known as R in the paper).\n",
    "    # It has dimensions |V| x K: vocab size x number of topics\n",
    "    decoder = model.decoder[0]\n",
    "    weights = decoder.weight.data.detach().clone()\n",
    "\n",
    "    for candidate in candidates:\n",
    "        test_word = PORTER_STEMMER.stem(candidate)\n",
    "        idx = vocab.stoi[test_word]\n",
    "        print(test_word, idx)\n",
    "\n",
    "        # Show top 10 most similar (based on cosine distance)\n",
    "        sims = F.cosine_similarity(weights[idx].unsqueeze(0), weights)\n",
    "        sim_vals, sim_idxs = torch.topk(sims, 15)\n",
    "\n",
    "        # Show ith nearest word and its score.\n",
    "        for i, v in zip(sim_idxs, sim_vals):\n",
    "            print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "\n",
    "        print(\"-----------\")\n",
    "    \n",
    "    V, K = weights.size()\n",
    "    for i in range(K):\n",
    "        print(f\"Topic {i+1}\")\n",
    "        vals, idxs = torch.topk(torch.abs(weights[:, i]), 30)\n",
    "        for i, v in zip(idxs, vals):\n",
    "            print(f\"{vocab.itos[i]}\\t{v.item()}\")\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic coherence.\n",
    "def umass_score(tf):\n",
    "    \"\"\" Compute topic coherence using UMass metric.\n",
    "    \n",
    "    Ref: http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "    \n",
    "    tf: term-frequency matrix for each document.\n",
    "        Each i^th row is the BOW representation of the i^th document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # D(wi): count of documents containing the word wi (i.e. df)\n",
    "    Dwi = np.array(np.sum(tf > 0, axis=0))[0]\n",
    "\n",
    "    W_bin = np.zeros_like(tf)\n",
    "    W_bin[tf > 0] = 1\n",
    "    \n",
    "    # D(wi, wj): count of documents containing both words wi and wj\n",
    "    Dwi_wj = W_bin.T @ W_bin\n",
    "\n",
    "    score_umass = np.log((Dwi_wj + 1)/ Dwi)\n",
    "    \n",
    "    return score_umass\n",
    "\n",
    "\n",
    "def topic_coherence(topic_vocab, n_top_words, pair_score):\n",
    "    \"\"\"\n",
    "    topic_vocab: dimensions (number of topics, vocabulary size).\n",
    "    model.components_ for LDA, and the \"semantic embedding\" matrix in the decoder for NVDM.\n",
    "    \n",
    "    pair_score: matrix of scores (e.g. UMass)\n",
    "    \"\"\"\n",
    "    coherences = []\n",
    "    for topic_idx, topic in enumerate(topic_vocab):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        coh = 0\n",
    "        for i in range(len(top_features_ind)):\n",
    "            for j in range(i):\n",
    "                coh += pair_score[top_features_ind[i], top_features_ind[j]]\n",
    "        coherences.append(coh)\n",
    "    return coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "MODELSAVE_PATH = \"./modelsaves\"\n",
    "\n",
    "models_k = dict()\n",
    "k_values = []\n",
    "\n",
    "for filename in os.listdir(MODELSAVE_PATH):\n",
    "    \n",
    "    num_topics = filename.split(\"_\")[1][1:]\n",
    "    num_topics = int(num_topics)\n",
    "    k_values.append(num_topics)\n",
    "    \n",
    "    model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, \"cpu\")\n",
    "    model.load_state_dict(torch.load(os.path.join(MODELSAVE_PATH, filename), map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    models_k[num_topics] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the BOW matrix manually, using the existing Vocab's token-to-index mapping.\n",
    "bow_mat = np.zeros((len(BDs), len(vocab)))\n",
    "for d, bd in enumerate(BDs):\n",
    "    token_idxs = vocab.lookup_indices(bd.split(\" \"))\n",
    "    word_counts = Counter(token_idxs)\n",
    "    for w, count in word_counts.items():\n",
    "        bow_mat[d, w] = count   \n",
    "bow_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_umass_mat = umass_score(bow_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values.sort()\n",
    "tc_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f'Running k = {k}')\n",
    "    this_model = models_k[k]\n",
    "    \n",
    "    # Extract the topic vocab matrix\n",
    "    decoder = this_model.decoder[0]\n",
    "    weights = decoder.weight.data.detach().clone().numpy()\n",
    "    topic_vocab_mat = weights.T\n",
    "    coherences = topic_coherence(topic_vocab_mat, 10, score_umass_mat)\n",
    "    this_c = np.median(coherences)\n",
    "\n",
    "    tc_values.append(this_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, tc_values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(topic_vocab, feature_names, n_top_words, title):\n",
    "    K = len(topic_vocab)\n",
    "    n_x = 5\n",
    "    n_y = int(np.ceil(K / n_x))\n",
    "    fig, axes = plt.subplots(n_y, n_x, figsize=(2.5 * n_x, 4 * n_y), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(topic_vocab):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 14})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [vocab.itos[i] for i in range(0, len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "this_model = models_k[k]\n",
    "decoder = this_model.decoder[0]\n",
    "weights = decoder.weight.data.detach().clone().numpy()\n",
    "topic_vocab_mat = weights.T\n",
    "\n",
    "plot_top_words(topic_vocab_mat, feature_names, 10, f\"NVDM K={k} Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, dataset, n_sample):\n",
    "    \"\"\" Compute perplexity of PyTorch model on the PyTorch Dataset. \"\"\"\n",
    "\n",
    "    # Iterate over the whole dataset once (1 big batch).\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=len(dataset),\n",
    "                             shuffle=False,\n",
    "                             collate_fn=collate_batch)\n",
    "    assert len(data_loader) == 1\n",
    "\n",
    "    for idx, (text, offsets) in enumerate(data_loader):\n",
    "        text = text.to(\"cpu\")\n",
    "        offsets = offsets.to(\"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            # Estimate the loss for each document by sampling\n",
    "            loss_dict = this_model(text, offsets, kl_weight=1.0, n_sample=n_sample)\n",
    "            loss = loss_dict[\"total\"]\n",
    "\n",
    "            # According to the NVDM paper, we estimate log probabilities using\n",
    "            # the variational lower bound (negative of loss).\n",
    "            log_probs = -loss\n",
    "\n",
    "            # Perplexity below...\n",
    "            \n",
    "            # Hack to get the number of words per document\n",
    "            # using the offsets (starting index of each document).\n",
    "            doc_lengths = torch.zeros(offsets.size(0))\n",
    "            doc_lengths[:-1] = offsets[1:] - offsets[:-1]\n",
    "            doc_lengths[-1] = text.size(0) - offsets[-1]\n",
    "            # # Sanity check\n",
    "            # assert torch.all(torch.tensor([len(bd.split(\" \")) for bd in BDs]) == doc_lengths)\n",
    "\n",
    "            # Average over the words for each document\n",
    "            ppx_doc = log_probs / doc_lengths\n",
    "\n",
    "            # Perplexity: argument averages over documents\n",
    "            ppx = torch.exp(-ppx_doc.mean()).item()\n",
    "\n",
    "            return ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity example\n",
    "k = 10\n",
    "this_model = models_k[k]\n",
    "this_model = this_model.to(\"cpu\")\n",
    "this_model.eval()\n",
    "\n",
    "perplexity(this_model, dataset, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVDM Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pp\n",
    "from pathlib import Path\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from models import nvdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Description Data\n",
    "### SP500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_dir = Path('../SP500')\n",
    "filenames = os.listdir(sp500_dir)\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ticker_sp50 = []\n",
    "sector_sp50 = []\n",
    "bds_sp50 = []\n",
    "for fn in filenames:\n",
    "    fn_first = fn.split('.txt')[0]\n",
    "    ticker, sector = fn_first.split('_')\n",
    "\n",
    "    path = os.path.join(sp500_dir, fn)\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        f_text = f.read()\n",
    "    \n",
    "    ticker_sp50.append(ticker)\n",
    "    sector_sp50.append(sector)\n",
    "    bds_sp50.append(f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic sanity checks\n",
    "\n",
    "# Just one file is named wrong\n",
    "if ticker_sp50[19] == \"VF Corporation\":\n",
    "    ticker_sp50[19] = \"VFC\"\n",
    "assert all(s.isupper() for s in ticker_sp50)\n",
    "assert all(s.title() == s for s in sector_sp50)\n",
    "assert all (len(bd) > 0 for bd in bds_sp50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Dataset - excluding SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we use the cleaned version of bds_1.txt.\n",
    "# Format is the same except the business descriptions are preprocessed.\n",
    "\n",
    "f = open(\"./data/bds_1_clean.txt\", \"r\", encoding=\"utf8\")\n",
    "f_lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "company_ids_all = f_lines[0::2]\n",
    "company_descriptions_all = f_lines[1::2]\n",
    "company_tickers = [x.split(':')[0] for x in company_ids_all]\n",
    "removeSP = np.in1d(np.array(company_tickers), list(ticker_sp50))\n",
    "\n",
    "bds_all = []\n",
    "ticker_all = []\n",
    "for i, d in enumerate(company_descriptions_all):\n",
    "    if (len(d) > 3000) and not removeSP[i]:\n",
    "        bds_all.append(d)\n",
    "        ticker_all.append(company_ids_all[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Returns Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_d = np.datetime64('2018-01-01')\n",
    "end_d = np.datetime64('2020-01-01')\n",
    "business_ds = pd.date_range(start_d, end_d, freq='B')\n",
    "\n",
    "price_data = pd.read_csv('./data/Price.csv')\n",
    "\n",
    "select_these = np.in1d(price_data.tic.values, list(ticker_sp50))\n",
    "price_sp50 = price_data.loc[select_these, ['tic', 'datadate', 'prccd']]\n",
    "price_sp50['datadate'] = pd.to_datetime(price_sp50['datadate'], format='%Y%m%d')\n",
    "price_sp50 = pd.pivot_table(price_sp50,index='datadate',columns='tic',values='prccd')\n",
    "price_sp50 = price_sp50.ffill(limit=5)\n",
    "price_sp50 = price_sp50.reindex(business_ds)\n",
    "price_sp50 = price_sp50.dropna(axis=0)\n",
    "\n",
    "returns_sp50 = np.log(price_sp50) - np.log(price_sp50.shift(1))\n",
    "returns_sp50 = returns_sp50.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Returns with NVDM\n",
    "### Get the pretrained NVDM torch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary object.\n",
    "with open(\"./vocabs/vocab_bds_1_clean_10000.pickle\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained NVDM models for different k.\n",
    "\n",
    "MODELSAVE_PATH = \"./modelsaves\"\n",
    "\n",
    "models_k = dict()\n",
    "k_values = []\n",
    "\n",
    "hidden_size = 500\n",
    "\n",
    "for filename in os.listdir(MODELSAVE_PATH):\n",
    "    \n",
    "    num_topics = filename.split(\"_\")[1][1:]\n",
    "    num_topics = int(num_topics)\n",
    "    k_values.append(num_topics)\n",
    "    \n",
    "    model = nvdm.NVDM(len(vocab), hidden_size, num_topics, 1, \"cpu\")\n",
    "    model.load_state_dict(torch.load(os.path.join(MODELSAVE_PATH, filename), map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    models_k[num_topics] = model\n",
    "    \n",
    "    # Hidden size was hard-coded in NVDM_Experiments, sanity check\n",
    "    assert model.n_hidden == hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_k[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features for S&P 500\n",
    "More manual work needed for NVDM (no fit/transform() functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvdm_transform(model, bow):\n",
    "    # replacing this_lda.transform(tf_sp50)\n",
    "    vocab_topic_mat = model.decoder[0].weight.data.detach().clone().numpy()\n",
    "    doc_topic_mat = bow @ vocab_topic_mat\n",
    "    return doc_topic_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the BOW matrix manually, using the existing Vocab's token-to-index mapping.\n",
    "bow_sp50 = np.zeros((len(bds_sp50), len(vocab)))\n",
    "\n",
    "for d, bd in enumerate(bds_sp50):\n",
    "    token_idxs = vocab.lookup_indices(bd.split(\" \"))\n",
    "    word_counts = Counter(token_idxs)\n",
    "    for w, count in word_counts.items():\n",
    "        bow_sp50[d, w] = count   \n",
    "bow_sp50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learned semantic embeddings from a model (e.g. K=10)\n",
    "vocab_topic_mat = models_k[10].decoder[0].weight.data.detach().clone().numpy()\n",
    "vocab_topic_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should represent each document as a linear combination of its word embeddings.\n",
    "# i.e. for each document, sum its word embeddings.\n",
    "doc_topic_mat = bow_sp50 @ vocab_topic_mat\n",
    "doc_topic_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the same way as LDA...\n",
    "features_sp50 = doc_topic_mat\n",
    "features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Over Dates and Perform OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = returns_sp50.index\n",
    "max_f = 19\n",
    "adj_r2_features = []\n",
    "\n",
    "for dd in all_dates:\n",
    "    reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "    y = reg_data[:, 0]\n",
    "    X = reg_data[:, 1:]\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    X = std_scaler.fit_transform(X)\n",
    "\n",
    "    X = sm.add_constant(X, prepend=False)\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    res = ols_model.fit()\n",
    "    adj_r2_features.append(res.rsquared_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(all_dates, adj_r2_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate average regression adjusted $R^2$ for different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = bow_sp50\n",
    "all_ks = list(sorted(models_k.keys()))\n",
    "\n",
    "all_adj_r2 = []\n",
    "for k in all_ks:\n",
    "    print(f'Running for k = {k}')\n",
    "    \n",
    "    features_sp50 = nvdm_transform(models_k[k], bow_sp50)\n",
    "    features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)\n",
    "    adj_r2_dates = []\n",
    "    for dd in all_dates:\n",
    "        reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "        y = reg_data[:, 0]\n",
    "        X = reg_data[:, 1:]\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "        X = std_scaler.fit_transform(X)\n",
    "\n",
    "        X = sm.add_constant(X, prepend=False)\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        res = ols_model.fit()\n",
    "        adj_r2_dates.append(res.rsquared_adj)\n",
    "    \n",
    "    all_adj_r2.append(np.mean(adj_r2_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
