{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pp\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Description Data\n",
    "## SP500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = Path('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/SP500')\n",
    "os.chdir(my_path)\n",
    "fns = os.listdir(my_path)\n",
    "\n",
    "ticker_sp50 = []\n",
    "sector_sp50 = []\n",
    "bds_sp50 = []\n",
    "for fn in fns:\n",
    "    fn_first = fn.split('.txt')[0]\n",
    "    ticker = fn_first.split('_')[0]\n",
    "    sector = fn_first.split('_')[1]\n",
    "    f = open(fn, 'r', encoding=\"utf8\")\n",
    "    f_text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    ticker_sp50.append(ticker)\n",
    "    sector_sp50.append(sector)\n",
    "    bds_sp50.append(f_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Dataset - excluding SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data'))\n",
    "f = open(\"bds_1.txt\", \"r\", encoding=\"utf8\")\n",
    "f_lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "company_ids_all = f_lines[0::2]\n",
    "company_descriptions_all = f_lines[1::2]\n",
    "company_tickers = [x.split(':')[0] for x in company_ids_all]\n",
    "removeSP = np.in1d(np.array(company_tickers), list(ticker_sp50))\n",
    "\n",
    "bds_all = []\n",
    "ticker_all = []\n",
    "for i, d in enumerate(company_descriptions_all):\n",
    "    if (len(d) > 3000) and not removeSP[i]:\n",
    "        bds_all.append(d)\n",
    "        ticker_all.append(company_ids_all[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Returns Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_d = np.datetime64('2018-01-01')\n",
    "end_d = np.datetime64('2020-01-01')\n",
    "business_ds = pd.date_range(start_d, end_d, freq='B')\n",
    "\n",
    "my_path = Path('/home/lawrence/Personal/Masters/COMP0087_ Natural_Language_Processing/Project/Data/MarketData')\n",
    "os.chdir(my_path)\n",
    "price_data = pd.read_csv('Price.csv')\n",
    "\n",
    "select_these = np.in1d(price_data.tic.values, list(ticker_sp50))\n",
    "price_sp50 = price_data.loc[select_these, ['tic', 'datadate', 'prccd']]\n",
    "price_sp50['datadate'] = pd.to_datetime(price_sp50['datadate'], format='%Y%m%d')\n",
    "price_sp50 = pd.pivot_table(price_sp50,index='datadate',columns='tic',values='prccd')\n",
    "price_sp50 = price_sp50.ffill(limit=5)\n",
    "price_sp50 = price_sp50.reindex(business_ds)\n",
    "price_sp50 = price_sp50.dropna(axis=0)\n",
    "\n",
    "returns_sp50 = np.log(price_sp50) - np.log(price_sp50.shift(1))\n",
    "returns_sp50 = returns_sp50.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Returns with LDA\n",
    "## Train LDA on Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(bds_all)\n",
    "\n",
    "n_components = 20\n",
    "lda_20 = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_20.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features for S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = tf_vectorizer.transform(bds_sp50)\n",
    "\n",
    "features_sp50 = lda_20.transform(tf_sp50)\n",
    "\n",
    "features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Over Dates and Perform OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = returns_sp50.index\n",
    "max_f = 19\n",
    "adj_r2_features = []\n",
    "\n",
    "for dd in all_dates:\n",
    "    reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "    y = reg_data[:, 0]\n",
    "    X = reg_data[:, 1:]\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    X = std_scaler.fit_transform(X)\n",
    "\n",
    "    X = sm.add_constant(X, prepend=False)\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    res = ols_model.fit()\n",
    "    adj_r2_features.append(res.rsquared_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(all_dates, adj_r2_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate average regression adjusted $R^2$ for different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sp50 = tf_vectorizer.transform(bds_sp50)\n",
    "all_ks = [5, 10, 15, 20, 25, 30, 40, 50]\n",
    "num_trials = 20\n",
    "all_adj_r2 = []\n",
    "for k in all_ks:\n",
    "    print(f'Running for k = {k}')\n",
    "    adj_r2_k = []\n",
    "    for t in range(num_trials):\n",
    "        this_lda = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.)\n",
    "        this_lda.fit(tf)\n",
    "    \n",
    "        features_sp50 = this_lda.transform(tf_sp50)\n",
    "        features_sp50_df = pd.DataFrame(index=ticker_sp50, data=features_sp50)\n",
    "        adj_r2_dates = []\n",
    "        for dd in all_dates:\n",
    "            reg_data = returns_sp50.loc[[dd]].transpose().join(features_sp50_df.loc[:, 0:max_f]).dropna(axis=0).values\n",
    "            y = reg_data[:, 0]\n",
    "            X = reg_data[:, 1:]\n",
    "\n",
    "            std_scaler = StandardScaler()\n",
    "            X = std_scaler.fit_transform(X)\n",
    "\n",
    "            X = sm.add_constant(X, prepend=False)\n",
    "            ols_model = sm.OLS(y, X)\n",
    "            res = ols_model.fit()\n",
    "            adj_r2_dates.append(res.rsquared_adj)\n",
    "        adj_r2_k.append(np.mean(adj_r2_dates))\n",
    "    all_adj_r2.append(adj_r2_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(x) for x in all_adj_r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.std(x) for x in all_adj_r2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
